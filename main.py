# main.py - FastAPI í†µí•© ì„œë²„ êµ¬í˜„
from fastapi import FastAPI, UploadFile, File, WebSocket, Form, WebSocketDisconnect, BackgroundTasks, HTTPException
from fastapi.staticfiles import StaticFiles
from fastapi.middleware.cors import CORSMiddleware
from fastapi.responses import FileResponse, JSONResponse
import uvicorn
import asyncio
from pathlib import Path
import ollama
import os
import uuid
import json
import re
import requests
from dotenv import load_dotenv

from database.db import (
    init_database,
    create_job,
    get_job,
    list_jobs,
    update_job_status,
    update_job_record,
    update_job_feedback,
    reset_feedback_state,
    delete_job,
    count_jobs,
)
from pydantic import BaseModel, Field
from typing import Optional, List
from config.settings import HOST, PORT
from confluence_api import get_page_content, get_child_pages, get_pages_recursively, combine_pages_content

# í™˜ê²½ ë³€ìˆ˜ ë¡œë“œ
load_dotenv()

app = FastAPI(title="AI Proposal Reviewer", version="1.0.0")

# CORS ì„¤ì • (MVP: ëª¨ë“  origin í—ˆìš©)
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# ì •ì  íŒŒì¼ ì„œë¹™ (í”„ë¡ íŠ¸ì—”ë“œ)
static_path = Path("static")
if static_path.exists():
    app.mount("/static", StaticFiles(directory="static"), name="static")

# WebSocket ì—°ê²° ê´€ë¦¬
active_connections: dict[str, WebSocket] = {}


class JobCreateRequest(BaseModel):
    title: Optional[str] = None
    proposal_content: str = Field(..., min_length=1)
    domain: str = Field(..., min_length=1)
    division: str = Field(..., min_length=1)
    status: str = "pending"
    human_decision: str = "pending"
    llm_decision: Optional[str] = None
    hitl_stages: Optional[List[int]] = None
    metadata: Optional[dict] = None


class JobUpdateRequest(BaseModel):
    title: Optional[str] = None
    proposal_content: Optional[str] = None
    domain: Optional[str] = None
    division: Optional[str] = None
    status: Optional[str] = None
    human_decision: Optional[str] = None
    llm_decision: Optional[str] = None
    hitl_stages: Optional[List[int]] = None
    metadata: Optional[dict] = None

def _extract_json_dict(text: str) -> Optional[dict]:
    if not text:
        return None
    try:
        parsed = json.loads(text)
        if isinstance(parsed, dict):
            return parsed
    except json.JSONDecodeError:
        pass

    match = re.search(r'\{.*\}', text, re.DOTALL)
    if match:
        try:
            parsed = json.loads(match.group())
            if isinstance(parsed, dict):
                return parsed
        except json.JSONDecodeError:
            return None
    return None


def _truncate_for_prompt(text: str, limit: int = 800) -> str:
    if not text:
        return ''
    text = text.strip()
    if len(text) <= limit:
        return text
    return text[:limit] + '...'


def _generate_title_sync(content: str, fallback: str) -> str:
    prompt_body = _truncate_for_prompt(content, 600)
    prompt = f"""
ë‹¹ì‹ ì€ ì œì•ˆì„œ ì œëª©ì„ ë§Œë“œëŠ” ì „ë¬¸ê°€ì…ë‹ˆë‹¤. ì•„ë˜ ì œì•ˆì„œ ë‚´ìš©ì„ ë³´ê³  í•µì‹¬ì„ í‘œí˜„í•˜ëŠ” 25ì ì´í•˜ì˜ í•œêµ­ì–´ ì œëª©ì„ ì‘ì„±í•˜ì„¸ìš”.
ì œëª©ì€ íŠ¹ìˆ˜ë¬¸ì ì—†ì´ ê°„ê²°í•˜ê²Œ ì‘ì„±í•˜ê³ , JSON í˜•ì‹ìœ¼ë¡œë§Œ ì‘ë‹µí•˜ì„¸ìš”.

ì œì•ˆì„œ:
{prompt_body}

ì‘ë‹µ í˜•ì‹:
{{"title": "ì—¬ê¸°ì— ì œëª©"}}
"""
    response = call_llm(prompt)
    data = _extract_json_dict(response)
    if data and isinstance(data.get('title'), str):
        title = data['title'].strip()
        if title:
            return title[:50]
    snippet_lines = (content or '').strip().splitlines()
    for line in snippet_lines:
        line = line.strip()
        if line:
            return line[:50]
    return fallback


async def generate_job_title(content: str, fallback: str) -> str:
    return await asyncio.to_thread(_generate_title_sync, content, fallback)


def _classify_decision_sync(final_report: str, final_recommendation: str) -> dict:
    prompt = f"""
ë‹¹ì‹ ì€ AI í”„ë¡œì íŠ¸ ì‹¬ì‚¬ìœ„ì›ì…ë‹ˆë‹¤. ìµœì¢… ë³´ê³ ì„œì™€ ìµœì¢… ì˜ê²¬ì„ ì½ê³  ê³¼ì œë¥¼ 'ìŠ¹ì¸' ë˜ëŠ” 'ë³´ë¥˜' ì¤‘ í•˜ë‚˜ë¡œ íŒë‹¨í•˜ì„¸ìš”.
ê²°ì • ê¸°ì¤€: ì‹¤í–‰ ê°€ëŠ¥ì„±, ê¸°ëŒ€ íš¨ê³¼, ë¦¬ìŠ¤í¬ ìˆ˜ì¤€, ROI ë“±ì„ ì¢…í•©ì ìœ¼ë¡œ ê³ ë ¤í•©ë‹ˆë‹¤.
ì¶œë ¥ì€ JSON í˜•ì‹ìœ¼ë¡œë§Œ ë‹µë³€í•˜ë©°, ê°€ëŠ¥í•œ ê°’ì€ "ìŠ¹ì¸" ë˜ëŠ” "ë³´ë¥˜"ì…ë‹ˆë‹¤.

ìµœì¢… ë³´ê³ ì„œ:
{_truncate_for_prompt(final_report, 1200)}

ìµœì¢… ì˜ê²¬:
{_truncate_for_prompt(final_recommendation, 800)}

ì‘ë‹µ í˜•ì‹ ì˜ˆì‹œ:
{{"decision": "ìŠ¹ì¸", "reason": "í•µì‹¬ ê·¼ê±°"}}
"""
    response = call_llm(prompt)
    data = _extract_json_dict(response) or {}
    decision = data.get('decision')
    if decision not in ('ìŠ¹ì¸', 'ë³´ë¥˜'):
        decision = 'ë³´ë¥˜'
    reason = data.get('reason') or 'LLM íŒë‹¨ì„ ê¸°ì¤€ìœ¼ë¡œ ìë™ ë¶„ë¥˜ë˜ì—ˆìŠµë‹ˆë‹¤.'
    return {'decision': decision, 'reason': reason}


async def classify_final_decision(final_report: str, final_recommendation: str) -> dict:
    return await asyncio.to_thread(_classify_decision_sync, final_report, final_recommendation)


def persist_job_metadata(job_id: int, new_status: str, agent_updates: dict | None = None, extra_updates: dict | None = None, **status_kwargs):
    job_snapshot = get_job(job_id) or {}
    metadata = job_snapshot.get("metadata", {}).copy()

    if agent_updates:
        agent_results = metadata.setdefault("agent_results", {})
        agent_results.update(agent_updates)

    if extra_updates:
        metadata.update(extra_updates)

    update_job_status(job_id, new_status, metadata=metadata, **status_kwargs)


@app.on_event("startup")
async def startup_event():
    """ì„œë²„ ì‹œì‘ ì‹œ ë°ì´í„°ë² ì´ìŠ¤ ë° LLM ì´ˆê¸°í™”"""
    print("Server starting...")
    init_database()
    print("Database ready")
    init_llm()
    print("LLM ready")

@app.get("/")
async def root():
    """ë£¨íŠ¸ ê²½ë¡œì—ì„œ index.html ì œê³µ"""
    return FileResponse("static/index.html")

@app.get("/health")
async def health_check():
    """í—¬ìŠ¤ ì²´í¬ ì—”ë“œí¬ì¸íŠ¸"""
    return {"status": "healthy", "service": "AI Proposal Reviewer"}

@app.post("/api/v1/review/submit")
async def submit_proposal(
    domain: str = Form(...),
    division: str = Form(...),
    hitl_stages: str = Form("[]"),
    file: UploadFile = File(None),
    text: str = Form(None)
):
    """
    ì œì•ˆì„œ ì œì¶œ - íŒŒì¼ ì—…ë¡œë“œ ë˜ëŠ” í…ìŠ¤íŠ¸ ì§ì ‘ ì…ë ¥
    """
    proposal_content = ""

    if file:
        # íŒŒì¼ ì—…ë¡œë“œ ë°©ì‹
        contents = await file.read()
        # MVP: ê°„ë‹¨í•œ í…ìŠ¤íŠ¸ ì¶”ì¶œ
        if file.filename.endswith(('.txt', '.md')):
            proposal_content = contents.decode('utf-8', errors='ignore')
        else:
            # PDF/DOCXëŠ” ì¶”í›„ ê³ ë„í™”
            proposal_content = contents.decode('utf-8', errors='ignore')
    elif text:
        # í…ìŠ¤íŠ¸ ì§ì ‘ ì…ë ¥ ë°©ì‹
        proposal_content = text
    else:
        return JSONResponse(
            status_code=400,
            content={"error": "íŒŒì¼ ë˜ëŠ” í…ìŠ¤íŠ¸ë¥¼ ì œê³µí•´ì£¼ì„¸ìš”"}
        )

    # HITL ë‹¨ê³„ íŒŒì‹±
    import json
    try:
        hitl_stages_list = json.loads(hitl_stages)
    except:
        hitl_stages_list = []  # ê¸°ë³¸ê°’: HITL ë¹„í™œì„±í™”

    # ì œëª© ìë™ ìƒì„± (LLM)
    generated_title = await generate_job_title(proposal_content, fallback=f"{domain} ì œì•ˆì„œ")

    # DBì— ì €ì¥í•˜ê³  job_id ìƒì„±
    job_id = create_job(
        proposal_content,
        domain,
        division,
        title=generated_title,
        hitl_stages=hitl_stages_list,
    )

    # ë°±ê·¸ë¼ìš´ë“œì—ì„œ ê²€í†  í”„ë¡œì„¸ìŠ¤ ì‹œì‘
    print(f"Starting background task for job {job_id}")
    asyncio.create_task(process_review(job_id))

    return {"job_id": job_id, "status": "submitted"}

# LLM ì„¤ì • ë° ì´ˆê¸°í™”
LLM_PROVIDER = os.getenv("LLM_PROVIDER", "ollama")
llm_client = None

def init_llm():
    """í™˜ê²½ ë³€ìˆ˜ì— ë”°ë¼ LLM í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™”"""
    global llm_client

    if LLM_PROVIDER == "internal":
        # Internal LLM ì„¤ì • (lazy import to avoid pydantic version issues)
        from langchain_openai import ChatOpenAI

        base_url = os.getenv("INTERNAL_BASE_URL")
        model = os.getenv("INTERNAL_MODEL")
        credential_key = os.getenv("INTERNAL_CREDENTIAL_KEY")
        system_name = os.getenv("INTERNAL_SYSTEM_NAME")
        user_id = os.getenv("INTERNAL_USER_ID")

        llm_client = ChatOpenAI(
            base_url=base_url,
            model=model,
            default_headers={
                "x-dep-ticket": credential_key,
                "Send-System-Name": system_name,
                "User-ID": user_id,
                "User-Type": "AD",
                "Prompt-Msg-Id": str(uuid.uuid4()),
                "Completion-Msg-Id": str(uuid.uuid4()),
            },
        )
        print(f"Internal LLM initialized: {model}")
    else:
        # Ollama ì„¤ì •
        llm_client = "ollama"  # OllamaëŠ” ì§ì ‘ í•¨ìˆ˜ë¡œ í˜¸ì¶œ
        print(f"Ollama LLM initialized: {os.getenv('OLLAMA_MODEL', 'gemma2:2b')}")

def call_llm(prompt: str) -> str:
    """í†µí•© LLM í˜¸ì¶œ í•¨ìˆ˜"""
    try:
        if LLM_PROVIDER == "internal":
            # Internal LLM ì‚¬ìš©
            response = llm_client.invoke(prompt)
            return response.content
        else:
            # Ollama ì‚¬ìš©
            model = os.getenv("OLLAMA_MODEL", "gemma2:2b")
            response = ollama.chat(
                model=model,
                messages=[{"role": "user", "content": prompt}]
            )
            return response['message']['content']
    except Exception as e:
        print(f"LLM API í˜¸ì¶œ ì‹¤íŒ¨: {e}")
        return f"AI ì‘ë‹µ ìƒì„± ì‹¤íŒ¨: {e}"

def call_ollama(prompt: str, model: str = "gemma3:1b") -> str:
    """Ollamaë¥¼ í†µí•œ LLM í˜¸ì¶œ (í•˜ìœ„ í˜¸í™˜ì„±ì„ ìœ„í•´ ìœ ì§€, ë‚´ë¶€ì ìœ¼ë¡œ call_llm ì‚¬ìš©)"""
    return call_llm(prompt)

def retrieve_from_rag(query_text: str, num_result_doc: int = 5, retrieval_method: str = "rrf") -> list:
    """RAGë¥¼ í†µí•œ ë¬¸ì„œ ê²€ìƒ‰

    Args:
        query_text: ê²€ìƒ‰ ì¿¼ë¦¬
        num_result_doc: ë°˜í™˜í•  ë¬¸ì„œ ìˆ˜
        retrieval_method: ê²€ìƒ‰ ë°©ë²• ("rrf", "bm25", "knn", "cc")

    Returns:
        ê²€ìƒ‰ ê²°ê³¼ ë¦¬ìŠ¤íŠ¸
    """
    try:
        # í™˜ê²½ ë³€ìˆ˜ì—ì„œ RAG ì„¤ì • ë¡œë“œ
        base_url = os.getenv("RAG_BASE_URL", "http://localhost:8000")
        credential_key = os.getenv("RAG_CREDENTIAL_KEY", "")
        rag_api_key = os.getenv("RAG_API_KEY", "")
        index_name = os.getenv("RAG_INDEX_NAME", "")
        permission_groups = os.getenv("RAG_PERMISSION_GROUPS", "user").split(",")

        # ê²€ìƒ‰ URL ì„¤ì •
        retrieval_urls = {
            "rrf": f"{base_url}/retrieve-rrf",
            "bm25": f"{base_url}/retrieve-bm25",
            "knn": f"{base_url}/retrieve-knn",
            "cc": f"{base_url}/retrieve-cc"
        }

        retrieval_url = retrieval_urls.get(retrieval_method, retrieval_urls["rrf"])

        # í—¤ë” ì„¤ì •
        headers = {
            "Content-Type": "application/json",
            "x-dep-ticket": credential_key,
            "api-key": rag_api_key
        }

        # ìš”ì²­ ë°ì´í„° ì„¤ì •
        fields = {
            "index_name": index_name,
            "permission_groups": permission_groups,
            "query_text": query_text,
            "num_result_doc": num_result_doc,
            "fields_exclude": ["v_merge_title_content"]
        }

        # RAG API í˜¸ì¶œ
        response = requests.post(retrieval_url, headers=headers, json=fields, timeout=30)

        if response.status_code == 200:
            result = response.json()
            print(f"RAG ê²€ìƒ‰ ì™„ë£Œ: {len(result.get('hits', {}).get('hits', []))}ê±´ ê²€ìƒ‰ë¨")
            return result.get('hits', {}).get('hits', [])
        else:
            print(f"RAG API í˜¸ì¶œ ì‹¤íŒ¨: {response.status_code} - {response.text}")
            return []

    except Exception as e:
        print(f"RAG ê²€ìƒ‰ ì‹¤íŒ¨: {e}")
        return []

def analyze_result_quality(agent_name: str, analysis_result: str, proposal_text: str) -> dict:
    """ì—ì´ì „íŠ¸ì˜ ë¶„ì„ ê²°ê³¼ í’ˆì§ˆì„ í‰ê°€í•˜ì—¬ ì¬ê²€í†  í•„ìš” ì—¬ë¶€ íŒë‹¨

    Returns:
        {
            "needs_retry": bool,  # ì¬ê²€í†  í•„ìš” ì—¬ë¶€
            "reason": str,  # ì¬ê²€í† ê°€ í•„ìš”í•œ ì´ìœ 
            "additional_info_needed": list  # í•„ìš”í•œ ì¶”ê°€ ì •ë³´ í•­ëª©ë“¤
        }
    """
    print(f"[DEBUG] Analyzing result quality for {agent_name}...")
    print(f"[DEBUG] Analysis result length: {len(analysis_result)}")

    quality_check_prompt = f"""ë‹¹ì‹ ì€ AI ê²€í†  í”„ë¡œì„¸ìŠ¤ì˜ í’ˆì§ˆ ê´€ë¦¬ orchestratorì…ë‹ˆë‹¤.
{agent_name}ê°€ ë‹¤ìŒê³¼ ê°™ì€ ë¶„ì„ ê²°ê³¼ë¥¼ ì œì¶œí–ˆìŠµë‹ˆë‹¤.

ì œì•ˆì„œ ë‚´ìš©:
{proposal_text[:500]}...

{agent_name}ì˜ ë¶„ì„ ê²°ê³¼:
{analysis_result}

ìœ„ ë¶„ì„ ê²°ê³¼ê°€ ì¶©ë¶„íˆ ìƒì„¸í•˜ê³  êµ¬ì²´ì ì¸ì§€ í‰ê°€í•´ì£¼ì„¸ìš”.

**ì¬ê²€í† ê°€ í•„ìš”í•œ ê²½ìš° (needs_retry = true):**
- ë¶„ì„ ë‚´ìš©ì´ ë„ˆë¬´ ì§§ê±°ë‚˜ ì¶”ìƒì ì¸ ê²½ìš° (2-3ë¬¸ì¥ ë¯¸ë§Œ)
- êµ¬ì²´ì ì¸ ê·¼ê±°ë‚˜ ë°ì´í„°ê°€ ë¶€ì¡±í•œ ê²½ìš°
- í•µì‹¬ ì§ˆë¬¸ì— ëŒ€í•œ ë‹µë³€ì´ ë¶ˆì¶©ë¶„í•œ ê²½ìš°
- "í‰ê°€ í•„ìš”", "ì¶”ê°€ ê²€í†  í•„ìš”" ë“± ëª¨í˜¸í•œ í‘œí˜„ë§Œ ìˆëŠ” ê²½ìš°
- ì œì•ˆì„œ ë‚´ìš©ì„ ì œëŒ€ë¡œ ë°˜ì˜í•˜ì§€ ì•Šì€ ê²½ìš°

**ì¬ê²€í† ê°€ ë¶ˆí•„ìš”í•œ ê²½ìš° (needs_retry = false):**
- ë¶„ì„ì´ ìƒì„¸í•˜ê³  êµ¬ì²´ì ì¸ ê²½ìš°
- ëª…í™•í•œ ê·¼ê±°ì™€ í•¨ê»˜ íŒë‹¨ì´ ì œì‹œëœ ê²½ìš°
- ìš”êµ¬ì‚¬í•­ì— ë§ê²Œ ì¶©ë¶„í•œ ì •ë³´ë¥¼ ì œê³µí•œ ê²½ìš°
- ê° í‰ê°€ í•­ëª©ì´ êµ¬ì²´ì ìœ¼ë¡œ ì„¤ëª…ëœ ê²½ìš°

ë°˜ë“œì‹œ ë‹¤ìŒ JSON í˜•ì‹ìœ¼ë¡œë§Œ ì‘ë‹µí•˜ì„¸ìš” (ì„¤ëª… ì—†ì´ JSONë§Œ):
{{
    "needs_retry": true,
    "reason": "ë¶„ì„ ë‚´ìš©ì´ ë„ˆë¬´ ê°„ëµí•˜ê³  êµ¬ì²´ì ì¸ ê·¼ê±°ê°€ ë¶€ì¡±í•¨",
    "additional_info_needed": ["êµ¬ì²´ì ì¸ ë°ì´í„°", "ìƒì„¸í•œ ê·¼ê±°", "ëª…í™•í•œ íŒë‹¨ ê¸°ì¤€"]
}}

ë˜ëŠ”

{{
    "needs_retry": false,
    "reason": "ë¶„ì„ì´ ì¶©ë¶„íˆ ìƒì„¸í•˜ê³  êµ¬ì²´ì ì„",
    "additional_info_needed": []
}}"""

    try:
        result = call_ollama(quality_check_prompt)
        print(f"[DEBUG] Raw quality check response: {result}")

        # JSON íŒŒì‹±
        import json
        json_str = result.strip()
        if "```json" in json_str:
            json_str = json_str.split("```json")[1].split("```")[0].strip()
        elif "```" in json_str:
            json_str = json_str.split("```")[1].split("```")[0].strip()

        print(f"[DEBUG] Extracted JSON string: {json_str}")
        analysis = json.loads(json_str)
        print(f"[DEBUG] Parsed quality analysis: {analysis}")

        # needs_retryê°€ booleanì´ ì•„ë‹ˆë©´ ë³€í™˜
        if isinstance(analysis.get("needs_retry"), str):
            analysis["needs_retry"] = analysis["needs_retry"].lower() in ["true", "yes", "1"]

        return analysis
    except Exception as e:
        print(f"[DEBUG] Failed to parse quality analysis: {e}")
        print(f"[DEBUG] Raw result was: {result if 'result' in locals() else 'No result'}")

        # íŒŒì‹± ì‹¤íŒ¨ ì‹œ ê°„ë‹¨í•œ íœ´ë¦¬ìŠ¤í‹± íŒë‹¨
        # ë¶„ì„ ê²°ê³¼ê°€ ë„ˆë¬´ ì§§ìœ¼ë©´ ì¬ì‹œë„
        if len(analysis_result.strip()) < 100:
            print(f"[DEBUG] Fallback: Analysis too short, enabling retry")
            return {"needs_retry": True, "reason": "ë¶„ì„ ê²°ê³¼ê°€ ë„ˆë¬´ ì§§ìŒ (100ì ë¯¸ë§Œ)", "additional_info_needed": ["ë” ìƒì„¸í•œ ë¶„ì„"]}

        return {"needs_retry": False, "reason": "Quality check failed - defaulting to no retry", "additional_info_needed": []}

def generate_feedback_suggestion(agent_name: str, analysis_result: str, proposal_text: str) -> str:
    """ì—ì´ì „íŠ¸ ë¶„ì„ ê²°ê³¼ë¥¼ ë°”íƒ•ìœ¼ë¡œ êµ¬ì²´ì ì¸ í”¼ë“œë°± ì œì•ˆ ìƒì„±"""
    print(f"[DEBUG] Generating feedback suggestion for {agent_name}...")
    feedback_prompt = f"""ë‹¹ì‹ ì€ AI ê³¼ì œ ì œì•ˆì„œ ê²€í†  ì „ë¬¸ê°€ì…ë‹ˆë‹¤.
ë‹¤ìŒì€ {agent_name}ì˜ ë¶„ì„ ê²°ê³¼ì…ë‹ˆë‹¤:

{analysis_result}

ì œì•ˆì„œ ì›ë¬¸:
{proposal_text}

ìœ„ ë¶„ì„ ê²°ê³¼ë¥¼ ë°”íƒ•ìœ¼ë¡œ, ì œì•ˆì„œ ì‘ì„±ìì—ê²Œ ì œê³µí•  êµ¬ì²´ì ì´ê³  ì‹¤í–‰ ê°€ëŠ¥í•œ í”¼ë“œë°±ì„ ì‘ì„±í•´ì£¼ì„¸ìš”.
í”¼ë“œë°±ì€ ë‹¤ìŒ í˜•ì‹ìœ¼ë¡œ ì‘ì„±í•´ì£¼ì„¸ìš”:

1. ê¸ì •ì ì¸ ë¶€ë¶„ (1-2ê°€ì§€)
2. ê°œì„ ì´ í•„ìš”í•œ ë¶€ë¶„ (2-3ê°€ì§€, êµ¬ì²´ì ì¸ ê°œì„  ë°©í–¥ í¬í•¨)
3. ì¶”ê°€ ê²€í†  ì‚¬í•­ (1-2ê°€ì§€)

ê° í•­ëª©ì€ bullet pointë¡œ ê°„ê²°í•˜ê²Œ ì‘ì„±í•´ì£¼ì„¸ìš”."""

    result = call_ollama(feedback_prompt)
    print(f"[DEBUG] Feedback suggestion generated (length: {len(result)} chars)")
    return result

async def rag_retrieve_bp_cases(domain: str, division: str):
    """RAGë¥¼ í†µí•œ BP ì‚¬ë¡€ ê²€ìƒ‰ (ì‹¤íŒ¨ ì‹œ fallback)"""
    try:
        # ê²€ìƒ‰ ì¿¼ë¦¬ êµ¬ì„±
        query_text = f"{domain} {division} AI ê³¼ì œ ì‚¬ë¡€ Best Practice"

        # RAG ê²€ìƒ‰ ìˆ˜í–‰ (ë¹„ë™ê¸° í•¨ìˆ˜ì—ì„œ ë™ê¸° í•¨ìˆ˜ í˜¸ì¶œ)
        rag_results = await asyncio.to_thread(retrieve_from_rag, query_text, num_result_doc=5)

        if rag_results:
            # RAG ê²€ìƒ‰ ê²°ê³¼ë¥¼ BP ì‚¬ë¡€ í˜•ì‹ìœ¼ë¡œ ë³€í™˜
            cases = []
            for hit in rag_results:
                source = hit.get('_source', {})
                case = {
                    "title": source.get('title', 'ì œëª© ì—†ìŒ'),
                    "tech_type": source.get('tech_type', 'AI/ML'),
                    "business_domain": source.get('business_domain', domain),
                    "division": source.get('division', division),
                    "problem_as_was": source.get('problem', 'ë¬¸ì œ ì •ì˜'),
                    "solution_to_be": source.get('solution', 'í•´ê²° ë°©ì•ˆ'),
                    "summary": source.get('summary', source.get('content', '')[:200]),
                    "tips": source.get('tips', '')
                }
                cases.append(case)

            print(f"RAG ê²€ìƒ‰ ì„±ê³µ: {len(cases)}ê±´ ë°˜í™˜")
            return {"success": True, "cases": cases}
        else:
            # RAG ê²€ìƒ‰ ì‹¤íŒ¨ ì‹œ ì‹œë®¬ë ˆì´ì…˜ ë°ì´í„° ë°˜í™˜
            print("RAG ê²€ìƒ‰ ê²°ê³¼ ì—†ìŒ, ì‹œë®¬ë ˆì´ì…˜ ë°ì´í„° ë°˜í™˜")
            return {
                "success": False,
                "cases": [
                    {"title": "AI ì¸í”„ë¼ ëª¨ë‹ˆí„°ë§", "tech_type": "AI/ML", "business_domain": domain, "division": division, "problem_as_was": "ìˆ˜ë™ ëª¨ë‹ˆí„°ë§ìœ¼ë¡œ ì¸í•œ ì¥ì•  ëŒ€ì‘ ì§€ì—°", "solution_to_be": "AI ê¸°ë°˜ ì‹¤ì‹œê°„ ì´ìƒ íƒì§€ ë° ìë™ ì•Œë¦¼", "summary": "AIë¥¼ í™œìš©í•œ ì¸í”„ë¼ ëª¨ë‹ˆí„°ë§ ìë™í™”", "tips": "ì‹¤ì‹œê°„ ë°ì´í„° ìˆ˜ì§‘ íŒŒì´í”„ë¼ì¸ êµ¬ì¶• í•„ìš”"},
                    {"title": "ì˜ˆì¸¡ ìœ ì§€ë³´ìˆ˜ ì‹œìŠ¤í…œ", "tech_type": "ì˜ˆì¸¡ ë¶„ì„", "business_domain": domain, "division": division, "problem_as_was": "ì‚¬í›„ ëŒ€ì‘ìœ¼ë¡œ ì¸í•œ ìƒì‚° ì¤‘ë‹¨", "solution_to_be": "ML ê¸°ë°˜ ê³ ì¥ ì˜ˆì¸¡ ë° ì‚¬ì „ ìœ ì§€ë³´ìˆ˜", "summary": "ì„¤ë¹„ ê³ ì¥ì„ ì‚¬ì „ì— ì˜ˆì¸¡í•˜ì—¬ ìƒì‚°ì„± í–¥ìƒ", "tips": "ê³¼ê±° ê³ ì¥ ë°ì´í„° ì¶•ì ì´ ì¤‘ìš”"},
                    {"title": "ì´ìƒ ê°ì§€ ìë™í™”", "tech_type": "ì´ìƒ íƒì§€", "business_domain": domain, "division": division, "problem_as_was": "í’ˆì§ˆ ë¶ˆëŸ‰ ë°œìƒ í›„ ì‚¬í›„ ì¡°ì¹˜", "solution_to_be": "ì‹¤ì‹œê°„ ì´ìƒ íƒì§€ ë° ìë™ ì¡°ì¹˜", "summary": "AI ê¸°ë°˜ í’ˆì§ˆ ì´ìƒ ìë™ ê°ì§€ ì‹œìŠ¤í…œ", "tips": "ì •ìƒ ë°ì´í„° íŒ¨í„´ í•™ìŠµì´ ì„ í–‰ë˜ì–´ì•¼ í•¨"}
                ]
            }
    except Exception as e:
        print(f"RAG retrieve failed (exception): {e}")
        # ì˜ˆì™¸ ë°œìƒ ì‹œ ì‹œë®¬ë ˆì´ì…˜ ë°ì´í„° ë°˜í™˜
        return {
            "success": False,
            "cases": [
                {"title": "AI ì¸í”„ë¼ ëª¨ë‹ˆí„°ë§", "tech_type": "AI/ML", "business_domain": domain, "division": division, "problem_as_was": "ìˆ˜ë™ ëª¨ë‹ˆí„°ë§ìœ¼ë¡œ ì¸í•œ ì¥ì•  ëŒ€ì‘ ì§€ì—°", "solution_to_be": "AI ê¸°ë°˜ ì‹¤ì‹œê°„ ì´ìƒ íƒì§€ ë° ìë™ ì•Œë¦¼", "summary": "AIë¥¼ í™œìš©í•œ ì¸í”„ë¼ ëª¨ë‹ˆí„°ë§ ìë™í™”", "tips": "ì‹¤ì‹œê°„ ë°ì´í„° ìˆ˜ì§‘ íŒŒì´í”„ë¼ì¸ êµ¬ì¶• í•„ìš”"},
                {"title": "ì˜ˆì¸¡ ìœ ì§€ë³´ìˆ˜ ì‹œìŠ¤í…œ", "tech_type": "ì˜ˆì¸¡ ë¶„ì„", "business_domain": domain, "division": division, "problem_as_was": "ì‚¬í›„ ëŒ€ì‘ìœ¼ë¡œ ì¸í•œ ìƒì‚° ì¤‘ë‹¨", "solution_to_be": "ML ê¸°ë°˜ ê³ ì¥ ì˜ˆì¸¡ ë° ì‚¬ì „ ìœ ì§€ë³´ìˆ˜", "summary": "ì„¤ë¹„ ê³ ì¥ì„ ì‚¬ì „ì— ì˜ˆì¸¡í•˜ì—¬ ìƒì‚°ì„± í–¥ìƒ", "tips": "ê³¼ê±° ê³ ì¥ ë°ì´í„° ì¶•ì ì´ ì¤‘ìš”"},
                {"title": "ì´ìƒ ê°ì§€ ìë™í™”", "tech_type": "ì´ìƒ íƒì§€", "business_domain": domain, "division": division, "problem_as_was": "í’ˆì§ˆ ë¶ˆëŸ‰ ë°œìƒ í›„ ì‚¬í›„ ì¡°ì¹˜", "solution_to_be": "ì‹¤ì‹œê°„ ì´ìƒ íƒì§€ ë° ìë™ ì¡°ì¹˜", "summary": "AI ê¸°ë°˜ í’ˆì§ˆ ì´ìƒ ìë™ ê°ì§€ ì‹œìŠ¤í…œ", "tips": "ì •ìƒ ë°ì´í„° íŒ¨í„´ í•™ìŠµì´ ì„ í–‰ë˜ì–´ì•¼ í•¨"}
            ]
        }

async def wait_for_feedback(job_id: int, timeout_seconds: int = 300):
    """HITL í”¼ë“œë°± ëŒ€ê¸° í—¬í¼ í•¨ìˆ˜"""
    print(f"Job {job_id}: Waiting for user feedback...")
    update_job_status(job_id, "waiting_feedback")

    for i in range(timeout_seconds):
        job = get_job(job_id)
        if job.get("status") == "feedback_received":
            print(f"Job {job_id}: Feedback received, continuing...")
            return True
        await asyncio.sleep(1)

    print(f"Job {job_id}: Timeout waiting for feedback, continuing anyway...")
    return False

async def process_confluence_pages_sequentially(job_ids: list, page_list: list):
    """Confluence í˜ì´ì§€ë“¤ì„ ìˆœì°¨ì ìœ¼ë¡œ ì²˜ë¦¬"""
    print(f"=== Sequential processing started for {len(job_ids)} pages ===")

    all_reports = []
    main_job_id = job_ids[0]  # ì²« ë²ˆì§¸ job_idë¥¼ ë©”ì¸ WebSocketìœ¼ë¡œ ì‚¬ìš©
    main_ws_key = str(main_job_id)  # WebSocket dictëŠ” ë¬¸ìì—´ í‚¤ë¥¼ ì‚¬ìš©

    for idx, job_id in enumerate(job_ids):
        page_info = page_list[idx]
        print(f"\n{'='*80}")
        print(f"Processing page {idx+1}/{len(job_ids)}: {page_info['title']} (Job ID: {job_id})")
        print(f"{'='*80}\n")

        # UIì— í˜„ì¬ ì²˜ë¦¬ ì¤‘ì¸ í˜ì´ì§€ ì•Œë¦¼
        if main_ws_key in active_connections:
            await active_connections[main_ws_key].send_json({
                "type": "page_progress",
                "current_page": idx + 1,
                "total_pages": len(job_ids),
                "page_title": page_info['title'],
                "page_id": page_info['id'],
                "job_id": job_id,
                "status": "processing",
                "message": f"ğŸ“„ í˜ì´ì§€ {idx+1}/{len(job_ids)} ë¶„ì„ ì¤‘: {page_info['title']}",
                "reset_agents": idx > 0
            })

        # ê° í˜ì´ì§€ ì²˜ë¦¬ (6ê°œ ì—ì´ì „íŠ¸ ì „ì²´ í”Œë¡œìš°)
        await process_review(
            job_id,
            ws_job_key=main_ws_key,
            send_final_report=False
        )

        # ì²˜ë¦¬ ì™„ë£Œëœ jobì˜ ìµœì¢… report ê°€ì ¸ì˜¤ê¸°
        job_data = get_job(job_id)
        if job_data and job_data.get('report'):
            all_reports.append({
                "page_title": page_info['title'],
                "page_id": page_info['id'],
                "job_id": job_id,
                "report": job_data['report'],
                "decision": job_data.get('llm_decision'),
                "decision_reason": (job_data.get('metadata') or {}).get('final_decision', {}).get('reason')
            })

        # UIì— í˜ì´ì§€ ì™„ë£Œ ì•Œë¦¼
        if main_ws_key in active_connections:
            await active_connections[main_ws_key].send_json({
                "type": "page_progress",
                "current_page": idx + 1,
                "total_pages": len(job_ids),
                "page_title": page_info['title'],
                "page_id": page_info['id'],
                "job_id": job_id,
                "status": "completed",
                "message": f"âœ… í˜ì´ì§€ {idx+1}/{len(job_ids)} ì™„ë£Œ: {page_info['title']}"
            })

        print(f"\nâœ… Completed page {idx+1}/{len(job_ids)}: {page_info['title']}\n")

    # ëª¨ë“  í˜ì´ì§€ ì²˜ë¦¬ ì™„ë£Œ í›„ í†µí•© ë¦¬í¬íŠ¸ ìƒì„±
    print(f"\n{'='*80}")
    print(f"All {len(job_ids)} pages processed successfully")
    print(f"{'='*80}\n")

    # ì²« ë²ˆì§¸ jobì˜ WebSocketìœ¼ë¡œ ìµœì¢… ì™„ë£Œ ì•Œë¦¼
    if job_ids and str(job_ids[0]) in active_connections:
        combined_report = "# ğŸ“š Confluence í˜ì´ì§€ë³„ ê²€í†  ê²°ê³¼\n\n"
        combined_report += f"**ì´ {len(all_reports)}ê°œ í˜ì´ì§€ ë¶„ì„ ì™„ë£Œ**\n\n"
        combined_report += "---\n\n"

        for idx, report_data in enumerate(all_reports, 1):
            combined_report += f"## {idx}. {report_data['page_title']}\n\n"
            combined_report += report_data['report']
            decision_line = report_data.get('decision')
            reason = report_data.get('decision_reason')
            if decision_line:
                combined_report += f"\n\n**ê²°ì •:** {decision_line}"
                if reason:
                    combined_report += f"<br>ì´ìœ : {reason}"
            combined_report += "\n\n---\n\n"

        decisions_summary = [
            {
                "page_title": item.get("page_title"),
                "decision": item.get("decision"),
                "reason": item.get("decision_reason"),
            }
            for item in all_reports
        ]

        await active_connections[str(job_ids[0])].send_json({
            "status": "completed",
            "report": combined_report,
            "page_count": len(all_reports),
            "decisions": decisions_summary,
        })

async def process_review(job_id: int, ws_job_key: str | None = None, send_final_report: bool = True):
    """ë°±ê·¸ë¼ìš´ë“œ ê²€í†  í”„ë¡œì„¸ìŠ¤ - 6ê°œ ì—ì´ì „íŠ¸ ì „ì²´ í”Œë¡œìš°"""
    print(f"=== process_review ENTRY for job {job_id} ===")
    ws = None
    ws_key = ws_job_key or str(job_id)
    try:
        print(f"process_review started for job {job_id}")
        job = get_job(job_id)
        print(f"Job data retrieved for job {job_id}")
        if not job:
            print(f"Job {job_id} not found")
            return

        # HITL ë‹¨ê³„ ì„¤ì • ê°€ì ¸ì˜¤ê¸°
        hitl_stages = job.get("hitl_stages", [])
        print(f"HITL stages enabled: {hitl_stages}")

        # HITL ì¬ì‹œë„ ì¹´ìš´í„° ì´ˆê¸°í™” (ê° ì—ì´ì „íŠ¸ë‹¹ ìµœëŒ€ 3íšŒ)
        hitl_retry_counts = {2: 0, 3: 0, 4: 0, 5: 0, 6: 0}
        MAX_HITL_RETRIES = 3

        # Wait for WebSocket connection (up to 3 seconds)
        for i in range(30):
            ws = active_connections.get(ws_key)
            if ws:
                print(f"WebSocket connected on attempt {i+1}")
                break
            await asyncio.sleep(0.1)

        print(f"WebSocket connection: {ws}")
        print(f"Active connections: {list(active_connections.keys())}")
        domain = job.get("domain", "")
        division = job.get("division", "")
        print(f"Domain: {domain}, Division: {division}")

        # Agent 1: BP Case Scouter
        if ws:
            await ws.send_json({"status": "processing", "agent": "BP_Scouter", "message": "BP ì‚¬ë¡€ ê²€ìƒ‰ ì¤‘..."})

        # RAG ê²€ìƒ‰ ì‹œë„ (ì‹¤íŒ¨í•´ë„ ê³„ì† ì§„í–‰)
        rag_result = await rag_retrieve_bp_cases(domain, division)
        bp_cases = rag_result.get("cases", [])

        await asyncio.sleep(2)
        if ws:
            # BP ê²€ìƒ‰ ì™„ë£Œ ë©”ì‹œì§€ì™€ í•¨ê»˜ ê²°ê³¼ ì „ì†¡
            await ws.send_json({
                "status": "completed",
                "agent": "BP_Scouter",
                "message": f"BP ì‚¬ë¡€ {len(bp_cases)}ê±´ ê²€ìƒ‰ ì™„ë£Œ",
                "bp_cases": bp_cases  # BP ê²€ìƒ‰ ê²°ê³¼ ì¶”ê°€
            })

        persist_job_metadata(
            job_id,
            "bp_scouter_done",
            agent_updates={"bp_scouter": bp_cases},
            extra_updates={"bp_cases": bp_cases},
        )

        # Agent 2: Objective Reviewer
        if ws:
            await ws.send_json({"status": "processing", "agent": "Objective_Reviewer", "message": "ëª©í‘œ ì í•©ì„± ê²€í†  ì¤‘..."})

        proposal_text = job.get("content", "")
        objective_prompt = f"""ë‹¹ì‹ ì€ ê¸°ì—…ì˜ AI ê³¼ì œ ì œì•ˆì„œë¥¼ ê²€í† í•˜ëŠ” ì „ë¬¸ê°€ì…ë‹ˆë‹¤.
ë‹¤ìŒ ì œì•ˆì„œì˜ ëª©í‘œ ì í•©ì„±ì„ ê²€í† í•˜ê³  í‰ê°€í•´ì£¼ì„¸ìš”:

ì œì•ˆì„œ ë‚´ìš©:
{proposal_text}

ë‹¤ìŒ í•­ëª©ì„ í‰ê°€í•˜ê³  ì§§ê²Œ ìš”ì•½í•´ì£¼ì„¸ìš”:
1. ëª©í‘œì˜ ëª…í™•ì„±
2. ì¡°ì§ ì „ëµê³¼ì˜ ì •ë ¬ì„±
3. ì‹¤í˜„ ê°€ëŠ¥ì„±

ê°„ê²°í•˜ê²Œ 2-3ë¬¸ì¥ìœ¼ë¡œ í‰ê°€ ê²°ê³¼ë¥¼ ì‘ì„±í•´ì£¼ì„¸ìš”."""

        objective_review = await asyncio.to_thread(call_ollama, objective_prompt)

        if ws:
            await ws.send_json({"status": "completed", "agent": "Objective_Reviewer", "message": "ëª©í‘œ ê²€í†  ì™„ë£Œ"})

        persist_job_metadata(
            job_id,
            "objective_done",
            agent_updates={"objective_review": objective_review},
        )

        # HITL ì¸í„°ëŸ½íŠ¸: Agent 2 ì´í›„ (ì„¤ì •ì— ë”°ë¼)
        if 2 in hitl_stages:
            agent_num = 2
            skip_accepted_agent2 = False
            while True:
                # LLMì´ ë¶„ì„ ê²°ê³¼ í’ˆì§ˆ í‰ê°€
                quality_check = await asyncio.to_thread(
                    analyze_result_quality,
                    "Objective Reviewer",
                    objective_review,
                    proposal_text
                )

                print(f"[DEBUG] Quality check for Agent 2: {quality_check}")

                # í”¼ë“œë°± ì œì•ˆ ìƒì„±
                feedback_suggestion = await asyncio.to_thread(
                    generate_feedback_suggestion,
                    "Objective Reviewer",
                    objective_review,
                    proposal_text
                )

                if ws:
                    await ws.send_json({
                        "status": "interrupt",
                        "job_id": job_id,
                        "message": f"ê²€í†  ê²°ê³¼ë¥¼ í™•ì¸í•´ì£¼ì„¸ìš” (ì¬ì‹œë„ {hitl_retry_counts[agent_num]}/{MAX_HITL_RETRIES}) - í’ˆì§ˆ: {quality_check.get('reason', '')}",
                        "results": {
                            "objective_review": objective_review,
                            "feedback_suggestion": feedback_suggestion,
                            "quality_check": quality_check
                        }
                    })

                # ì‚¬ìš©ìê°€ ê²°ê³¼ë¥¼ í™•ì¸í•  ë•Œê¹Œì§€ ëŒ€ê¸°
                await wait_for_feedback(job_id)

                # ì‚¬ìš©ì í”¼ë“œë°± ê°€ì ¸ì˜¤ê¸°
                updated_job = get_job(job_id)
                skip_requested = updated_job.get("feedback_skip", False)
                user_feedback = (updated_job.get("feedback") or "").strip()

                print(f"[DEBUG] User feedback retrieved: '{user_feedback}' (skip={skip_requested})")

                if skip_requested:
                    skip_accepted_agent2 = True
                    retry_decision = {"needs_retry": False, "reason": "ì‚¬ìš©ìê°€ ê±´ë„ˆë›°ê¸°ë¥¼ ì„ íƒ"}
                    reset_feedback_state(job_id)
                    print(f"[DEBUG] HITL skip acknowledged for Agent 2 (job {job_id})")
                elif user_feedback:
                    retry_decision = {"needs_retry": False, "reason": "ì‚¬ìš©ì í”¼ë“œë°± ë°˜ì˜"}
                    reset_feedback_state(job_id)
                else:
                    # í”¼ë“œë°±ì´ ì—†ìœ¼ë©´ í’ˆì§ˆ ê²€ì‚¬ ê²°ê³¼ ì‚¬ìš©
                    retry_decision = quality_check

                print(f"[DEBUG] Retry decision for Agent 2: {retry_decision}")

                if skip_accepted_agent2:
                    break

                # ì¬ì‹œë„ í•„ìš” ì—¬ë¶€ íŒë‹¨
                if retry_decision.get("needs_retry") and hitl_retry_counts[agent_num] < MAX_HITL_RETRIES:
                    hitl_retry_counts[agent_num] += 1
                    print(f"[DEBUG] Agent 2 ì¬ì‹œë„ {hitl_retry_counts[agent_num]}/{MAX_HITL_RETRIES}")

                    if ws:
                        await ws.send_json({
                            "status": "processing",
                            "agent": "Objective_Reviewer",
                            "message": f"í’ˆì§ˆ ê°œì„ ì„ ìœ„í•´ ì¬ê²€í†  ì¤‘... ({hitl_retry_counts[agent_num]}/{MAX_HITL_RETRIES})"
                        })

                    # í’ˆì§ˆ ê²€ì‚¬ ê²°ê³¼ë¥¼ ë°˜ì˜í•˜ì—¬ ì¬ê²€í† 
                    retry_prompt = f"""ë‹¹ì‹ ì€ ê¸°ì—…ì˜ AI ê³¼ì œ ì œì•ˆì„œë¥¼ ê²€í† í•˜ëŠ” ì „ë¬¸ê°€ì…ë‹ˆë‹¤.
ì´ì „ ê²€í†  ê²°ê³¼ê°€ í’ˆì§ˆ ê¸°ì¤€ì„ ì¶©ì¡±í•˜ì§€ ëª»í–ˆìŠµë‹ˆë‹¤. ë” ìƒì„¸í•˜ê³  êµ¬ì²´ì ìœ¼ë¡œ ì¬ê²€í† í•´ì£¼ì„¸ìš”.

ì œì•ˆì„œ ë‚´ìš©:
{proposal_text}

ì´ì „ ê²€í†  ê²°ê³¼ (ë¶ˆì¶©ë¶„):
{objective_review}

í’ˆì§ˆ ê²€ì‚¬ ê²°ê³¼:
- ë¬¸ì œì : {retry_decision.get('reason', 'ë¶„ì„ì´ ë¶ˆì¶©ë¶„í•¨')}
- ë³´ì™„ í•„ìš” ì‚¬í•­: {', '.join(retry_decision.get('additional_info_needed', ['ë” ìƒì„¸í•œ ë¶„ì„', 'êµ¬ì²´ì ì¸ ê·¼ê±°', 'ëª…í™•í•œ íŒë‹¨']))}

ìœ„ ë¬¸ì œì ì„ í•´ê²°í•˜ê³  ë‹¤ìŒ í•­ëª©ì„ **êµ¬ì²´ì ì´ê³  ìƒì„¸í•˜ê²Œ** ì¬í‰ê°€í•´ì£¼ì„¸ìš”:
1. ëª©í‘œì˜ ëª…í™•ì„± (ì œì•ˆì„œì— ëª…ì‹œëœ êµ¬ì²´ì ì¸ ëª©í‘œ ì¸ìš©)
2. ì¡°ì§ ì „ëµê³¼ì˜ ì •ë ¬ì„± (ì–´ë–¤ ì „ëµ ëª©í‘œì™€ ì–´ë–»ê²Œ ì—°ê²°ë˜ëŠ”ì§€)
3. ì‹¤í˜„ ê°€ëŠ¥ì„± (êµ¬ì²´ì ì¸ ê°€ëŠ¥/ë¶ˆê°€ëŠ¥ ê·¼ê±°)

**ë°˜ë“œì‹œ 5-7ë¬¸ì¥ ì´ìƒìœ¼ë¡œ êµ¬ì²´ì ì¸ ê·¼ê±°ì™€ í•¨ê»˜ í‰ê°€ ê²°ê³¼ë¥¼ ì‘ì„±í•´ì£¼ì„¸ìš”.**
ê° í•­ëª©ë§ˆë‹¤ ëª…í™•í•œ íŒë‹¨ê³¼ ê·¸ ì´ìœ ë¥¼ ì œì‹œí•˜ì„¸ìš”."""

                    objective_review = await asyncio.to_thread(call_ollama, retry_prompt)

                    if ws:
                        await ws.send_json({
                            "status": "completed",
                            "agent": "Objective_Reviewer",
                            "message": "ì¬ê²€í†  ì™„ë£Œ"
                        })

                    # ì¬ê²€í†  ê²°ê³¼ë¡œ ë‹¤ì‹œ HITL
                    continue
                else:
                    # ì¬ì‹œë„ ë¶ˆí•„ìš”í•˜ê±°ë‚˜ ìµœëŒ€ íšŸìˆ˜ ë„ë‹¬
                    if hitl_retry_counts[agent_num] >= MAX_HITL_RETRIES:
                        print(f"[DEBUG] Agent 2 ìµœëŒ€ ì¬ì‹œë„ íšŸìˆ˜ ë„ë‹¬")
                        if ws:
                            await ws.send_json({
                                "status": "processing",
                                "agent": "Objective_Reviewer",
                                "message": "ìµœëŒ€ ì¬ì‹œë„ íšŸìˆ˜ ë„ë‹¬, ë‹¤ìŒ ë‹¨ê³„ë¡œ ì§„í–‰í•©ë‹ˆë‹¤"
                            })
                    break

            # í”¼ë“œë°± ë°›ì€ í›„ ê³„ì† ì§„í–‰
            if ws:
                next_message = (
                    "ì‚¬ìš©ì ê±´ë„ˆë›°ê¸° ìš”ì²­ì„ ìˆ˜ë½í–ˆìŠµë‹ˆë‹¤. ë‹¤ìŒ ë‹¨ê³„ë¡œ ì§„í–‰í•©ë‹ˆë‹¤."
                    if skip_accepted_agent2
                    else "í”¼ë“œë°± ë°˜ì˜í•˜ì—¬ ë¶„ì„ ê³„ì† ì§„í–‰..."
                )
                await ws.send_json({
                    "status": "processing",
                    "agent": "Data_Analyzer",
                    "message": next_message
                })
            await asyncio.sleep(1)

        # Agent 3: Data Analyzer
        if ws:
            await ws.send_json({"status": "processing", "agent": "Data_Analyzer", "message": "ë°ì´í„° ë¶„ì„ ì¤‘..."})

        data_prompt = f"""ë‹¹ì‹ ì€ AI í”„ë¡œì íŠ¸ì˜ ë°ì´í„° ë¶„ì„ ì „ë¬¸ê°€ì…ë‹ˆë‹¤.
ë‹¤ìŒ ì œì•ˆì„œì— ëŒ€í•œ ë°ì´í„° ë¶„ì„ì„ ìˆ˜í–‰í•´ì£¼ì„¸ìš”:

ì œì•ˆì„œ ë‚´ìš©:
{proposal_text}

ë‹¤ìŒ í•­ëª©ì„ í‰ê°€í•˜ê³  ì§§ê²Œ ìš”ì•½í•´ì£¼ì„¸ìš”:
1. ë°ì´í„° í™•ë³´ ê°€ëŠ¥ì„±
2. ë°ì´í„° í’ˆì§ˆ ì˜ˆìƒ
3. ë°ì´í„° ì ‘ê·¼ì„±

ê°„ê²°í•˜ê²Œ 2-3ë¬¸ì¥ìœ¼ë¡œ í‰ê°€ ê²°ê³¼ë¥¼ ì‘ì„±í•´ì£¼ì„¸ìš”."""

        data_analysis = await asyncio.to_thread(call_ollama, data_prompt)

        if ws:
            await ws.send_json({"status": "completed", "agent": "Data_Analyzer", "message": "ë°ì´í„° ë¶„ì„ ì™„ë£Œ"})

        persist_job_metadata(
            job_id,
            "data_analyzer_done",
            agent_updates={"data_analysis": data_analysis},
        )

        # HITL ì¸í„°ëŸ½íŠ¸: Agent 3 ì´í›„ (ì„¤ì •ì— ë”°ë¼)
        if 3 in hitl_stages:
            agent_num = 3
            skip_accepted_agent3 = False
            while True:
                # LLMì´ ë¶„ì„ ê²°ê³¼ í’ˆì§ˆ í‰ê°€
                quality_check = await asyncio.to_thread(
                    analyze_result_quality,
                    "Data Analyzer",
                    data_analysis,
                    proposal_text
                )

                print(f"[DEBUG] Quality check for Agent 3: {quality_check}")

                feedback_suggestion = await asyncio.to_thread(
                    generate_feedback_suggestion,
                    "Data Analyzer",
                    data_analysis,
                    proposal_text
                )

                if ws:
                    await ws.send_json({
                        "status": "interrupt",
                        "job_id": job_id,
                        "message": f"ë°ì´í„° ë¶„ì„ ê²°ê³¼ í™•ì¸ ì¤‘... (ì¬ì‹œë„ {hitl_retry_counts[agent_num]}/{MAX_HITL_RETRIES}) - {quality_check.get('reason', '')}",
                        "results": {
                            "data_analysis": data_analysis,
                            "feedback_suggestion": feedback_suggestion,
                            "quality_check": quality_check
                        }
                    })

                # ì‚¬ìš©ìê°€ ê²°ê³¼ë¥¼ í™•ì¸í•  ë•Œê¹Œì§€ ëŒ€ê¸°
                await wait_for_feedback(job_id)

                # ì‚¬ìš©ì í”¼ë“œë°± ê°€ì ¸ì˜¤ê¸°
                updated_job = get_job(job_id)
                skip_requested = updated_job.get("feedback_skip", False)
                user_feedback = (updated_job.get("feedback") or "").strip()

                print(f"[DEBUG] User feedback retrieved: '{user_feedback}' (skip={skip_requested})")

                if skip_requested:
                    skip_accepted_agent3 = True
                    retry_decision = {"needs_retry": False, "reason": "ì‚¬ìš©ìê°€ ê±´ë„ˆë›°ê¸°ë¥¼ ì„ íƒ"}
                    reset_feedback_state(job_id)
                    print(f"[DEBUG] HITL skip acknowledged for Agent 3 (job {job_id})")
                elif user_feedback:
                    retry_decision = {"needs_retry": False, "reason": "ì‚¬ìš©ì í”¼ë“œë°± ë°˜ì˜"}
                    reset_feedback_state(job_id)
                else:
                    # í”¼ë“œë°±ì´ ì—†ìœ¼ë©´ í’ˆì§ˆ ê²€ì‚¬ ê²°ê³¼ ì‚¬ìš©
                    retry_decision = quality_check

                print(f"[DEBUG] Retry decision for Agent 3: {retry_decision}")

                if skip_accepted_agent3:
                    break

                if retry_decision.get("needs_retry") and hitl_retry_counts[agent_num] < MAX_HITL_RETRIES:
                    hitl_retry_counts[agent_num] += 1
                    print(f"[DEBUG] Agent 3 ì¬ì‹œë„ {hitl_retry_counts[agent_num]}/{MAX_HITL_RETRIES}")

                    if ws:
                        await ws.send_json({
                            "status": "processing",
                            "agent": "Data_Analyzer",
                            "message": f"í’ˆì§ˆ ê°œì„ ì„ ìœ„í•´ ì¬ê²€í†  ì¤‘... ({hitl_retry_counts[agent_num]}/{MAX_HITL_RETRIES})"
                        })

                    retry_prompt = f"""ë‹¹ì‹ ì€ AI í”„ë¡œì íŠ¸ì˜ ë°ì´í„° ë¶„ì„ ì „ë¬¸ê°€ì…ë‹ˆë‹¤.
ì´ì „ ë¶„ì„ ê²°ê³¼ê°€ í’ˆì§ˆ ê¸°ì¤€ì„ ì¶©ì¡±í•˜ì§€ ëª»í–ˆìŠµë‹ˆë‹¤. ë” ìƒì„¸í•˜ê³  êµ¬ì²´ì ìœ¼ë¡œ ì¬ë¶„ì„í•´ì£¼ì„¸ìš”.

ì œì•ˆì„œ ë‚´ìš©:
{proposal_text}

ì´ì „ ë¶„ì„ ê²°ê³¼ (ë¶ˆì¶©ë¶„):
{data_analysis}

í’ˆì§ˆ ê²€ì‚¬ ê²°ê³¼:
- ë¬¸ì œì : {retry_decision.get('reason', 'ë¶„ì„ì´ ë¶ˆì¶©ë¶„í•¨')}
- ë³´ì™„ í•„ìš” ì‚¬í•­: {', '.join(retry_decision.get('additional_info_needed', ['ë” ìƒì„¸í•œ ë¶„ì„', 'êµ¬ì²´ì ì¸ ê·¼ê±°']))}

ìœ„ ë¬¸ì œì ì„ í•´ê²°í•˜ê³  ë‹¤ìŒ í•­ëª©ì„ **êµ¬ì²´ì ì´ê³  ìƒì„¸í•˜ê²Œ** ì¬í‰ê°€í•´ì£¼ì„¸ìš”:
1. ë°ì´í„° í™•ë³´ ê°€ëŠ¥ì„± (ì–´ë–¤ ë°ì´í„°ê°€ í•„ìš”í•˜ê³ , ì–´ë””ì„œ í™•ë³´ ê°€ëŠ¥í•œì§€)
2. ë°ì´í„° í’ˆì§ˆ ì˜ˆìƒ (í’ˆì§ˆ ìˆ˜ì¤€ê³¼ ê·¸ ê·¼ê±°)
3. ë°ì´í„° ì ‘ê·¼ì„± (ì ‘ê·¼ ë°©ë²•ê³¼ ì œì•½ì‚¬í•­)

**ë°˜ë“œì‹œ 5-7ë¬¸ì¥ ì´ìƒìœ¼ë¡œ êµ¬ì²´ì ì¸ ê·¼ê±°ì™€ í•¨ê»˜ í‰ê°€ ê²°ê³¼ë¥¼ ì‘ì„±í•´ì£¼ì„¸ìš”.**"""

                    data_analysis = await asyncio.to_thread(call_ollama, retry_prompt)

                    if ws:
                        await ws.send_json({
                            "status": "completed",
                            "agent": "Data_Analyzer",
                            "message": "ì¬ë¶„ì„ ì™„ë£Œ"
                        })

                    continue
                else:
                    if hitl_retry_counts[agent_num] >= MAX_HITL_RETRIES:
                        print(f"[DEBUG] Agent 3 ìµœëŒ€ ì¬ì‹œë„ íšŸìˆ˜ ë„ë‹¬")
                        if ws:
                            await ws.send_json({
                                "status": "processing",
                                "agent": "Data_Analyzer",
                                "message": "ìµœëŒ€ ì¬ì‹œë„ íšŸìˆ˜ ë„ë‹¬, ë‹¤ìŒ ë‹¨ê³„ë¡œ ì§„í–‰í•©ë‹ˆë‹¤"
                            })
                    break

            if ws:
                next_message = (
                    "ì‚¬ìš©ì ê±´ë„ˆë›°ê¸° ìš”ì²­ì„ ìˆ˜ë½í–ˆìŠµë‹ˆë‹¤. ë‹¤ìŒ ë‹¨ê³„ë¡œ ì§„í–‰í•©ë‹ˆë‹¤."
                    if skip_accepted_agent3
                    else "í”¼ë“œë°± ë°˜ì˜í•˜ì—¬ ë¶„ì„ ê³„ì† ì§„í–‰..."
                )
                await ws.send_json({
                    "status": "processing",
                    "agent": "Risk_Analyzer",
                    "message": next_message
                })
            await asyncio.sleep(1)

        # Agent 4: Risk Analyzer
        if ws:
            await ws.send_json({"status": "processing", "agent": "Risk_Analyzer", "message": "ë¦¬ìŠ¤í¬ ë¶„ì„ ì¤‘..."})

        risk_prompt = f"""ë‹¹ì‹ ì€ AI í”„ë¡œì íŠ¸ì˜ ë¦¬ìŠ¤í¬ ë¶„ì„ ì „ë¬¸ê°€ì…ë‹ˆë‹¤.
ë‹¤ìŒ ì œì•ˆì„œì— ëŒ€í•œ ë¦¬ìŠ¤í¬ ë¶„ì„ì„ ìˆ˜í–‰í•´ì£¼ì„¸ìš”:

ì œì•ˆì„œ ë‚´ìš©:
{proposal_text}

ë‹¤ìŒ ë¦¬ìŠ¤í¬ë¥¼ í‰ê°€í•˜ê³  ê°ê° ì§§ê²Œ ìš”ì•½í•´ì£¼ì„¸ìš”:
1. ê¸°ìˆ ì  ë¦¬ìŠ¤í¬
2. ì¼ì • ë¦¬ìŠ¤í¬
3. ì¸ë ¥ ë¦¬ìŠ¤í¬

ê° í•­ëª©ë§ˆë‹¤ 1-2ë¬¸ì¥ìœ¼ë¡œ í‰ê°€ ê²°ê³¼ë¥¼ ì‘ì„±í•´ì£¼ì„¸ìš”."""

        risk_analysis = await asyncio.to_thread(call_ollama, risk_prompt)

        if ws:
            await ws.send_json({"status": "completed", "agent": "Risk_Analyzer", "message": "ë¦¬ìŠ¤í¬ ë¶„ì„ ì™„ë£Œ"})

        persist_job_metadata(
            job_id,
            "risk_done",
            agent_updates={"risk_analysis": risk_analysis},
        )

        # HITL ì¸í„°ëŸ½íŠ¸: Agent 4 ì´í›„ (ì„¤ì •ì— ë”°ë¼)
        if 4 in hitl_stages:
            agent_num = 4
            skip_accepted_agent4 = False
            while True:
                quality_check = await asyncio.to_thread(
                    analyze_result_quality,
                    "Risk Analyzer",
                    risk_analysis,
                    proposal_text
                )
                print(f"[DEBUG] Quality check for Agent 4: {quality_check}")

                feedback_suggestion = await asyncio.to_thread(
                    generate_feedback_suggestion,
                    "Risk Analyzer",
                    risk_analysis,
                    proposal_text
                )

                if ws:
                    await ws.send_json({
                        "status": "interrupt",
                        "job_id": job_id,
                        "message": f"ë¦¬ìŠ¤í¬ ë¶„ì„ ê²°ê³¼ í™•ì¸ ì¤‘... (ì¬ì‹œë„ {hitl_retry_counts[agent_num]}/{MAX_HITL_RETRIES}) - {quality_check.get('reason', '')}",
                        "results": {
                            "risk_analysis": risk_analysis,
                            "feedback_suggestion": feedback_suggestion,
                            "quality_check": quality_check
                        }
                    })

                # ì‚¬ìš©ìê°€ ê²°ê³¼ë¥¼ í™•ì¸í•  ë•Œê¹Œì§€ ëŒ€ê¸°
                await wait_for_feedback(job_id)

                updated_job = get_job(job_id)
                skip_requested = updated_job.get("feedback_skip", False)
                user_feedback = (updated_job.get("feedback") or "").strip()

                print(f"[DEBUG] User feedback retrieved (Agent 4): '{user_feedback}' (skip={skip_requested})")

                if skip_requested:
                    skip_accepted_agent4 = True
                    retry_decision = {"needs_retry": False, "reason": "ì‚¬ìš©ìê°€ ê±´ë„ˆë›°ê¸°ë¥¼ ì„ íƒ"}
                    reset_feedback_state(job_id)
                    print(f"[DEBUG] HITL skip acknowledged for Agent 4 (job {job_id})")
                elif user_feedback:
                    retry_decision = {"needs_retry": False, "reason": "ì‚¬ìš©ì í”¼ë“œë°± ë°˜ì˜"}
                    reset_feedback_state(job_id)
                else:
                    retry_decision = quality_check

                print(f"[DEBUG] Retry decision for Agent 4: {retry_decision}")

                if skip_accepted_agent4:
                    break

                if retry_decision.get("needs_retry") and hitl_retry_counts[agent_num] < MAX_HITL_RETRIES:
                    hitl_retry_counts[agent_num] += 1
                    print(f"[DEBUG] Agent 4 ì¬ì‹œë„ {hitl_retry_counts[agent_num]}/{MAX_HITL_RETRIES}")

                    if ws:
                        await ws.send_json({
                            "status": "processing",
                            "agent": "Risk_Analyzer",
                            "message": f"í’ˆì§ˆ ê°œì„ ì„ ìœ„í•´ ì¬ê²€í†  ì¤‘... ({hitl_retry_counts[agent_num]}/{MAX_HITL_RETRIES})"
                        })

                    retry_prompt = f"""ë‹¹ì‹ ì€ AI í”„ë¡œì íŠ¸ì˜ ë¦¬ìŠ¤í¬ ë¶„ì„ ì „ë¬¸ê°€ì…ë‹ˆë‹¤.
ì´ì „ ë¶„ì„ ê²°ê³¼ê°€ í’ˆì§ˆ ê¸°ì¤€ì„ ì¶©ì¡±í•˜ì§€ ëª»í–ˆìŠµë‹ˆë‹¤. ë” ìƒì„¸í•˜ê³  êµ¬ì²´ì ìœ¼ë¡œ ì¬ë¶„ì„í•´ì£¼ì„¸ìš”.

ì œì•ˆì„œ ë‚´ìš©:
{proposal_text}

ì´ì „ ë¶„ì„ ê²°ê³¼ (ë¶ˆì¶©ë¶„):
{risk_analysis}

í’ˆì§ˆ ê²€ì‚¬ ê²°ê³¼:
- ë¬¸ì œì : {retry_decision.get('reason', 'ë¶„ì„ì´ ë¶ˆì¶©ë¶„í•¨')}
- ë³´ì™„ í•„ìš” ì‚¬í•­: {', '.join(retry_decision.get('additional_info_needed', ['ë” ìƒì„¸í•œ ë¶„ì„']))}

ìœ„ ë¬¸ì œì ì„ í•´ê²°í•˜ê³  ë‹¤ìŒ ë¦¬ìŠ¤í¬ë¥¼ **êµ¬ì²´ì ì´ê³  ìƒì„¸í•˜ê²Œ** ì¬í‰ê°€í•´ì£¼ì„¸ìš”:
1. ê¸°ìˆ ì  ë¦¬ìŠ¤í¬ (êµ¬ì²´ì ì¸ ê¸°ìˆ  ë¬¸ì œì ê³¼ ì˜í–¥)
2. ì¼ì • ë¦¬ìŠ¤í¬ (ì§€ì—° ê°€ëŠ¥ì„±ê³¼ ì›ì¸)
3. ì¸ë ¥ ë¦¬ìŠ¤í¬ (í•„ìš” ì—­ëŸ‰ê³¼ í™•ë³´ ê°€ëŠ¥ì„±)

**ë°˜ë“œì‹œ 5-7ë¬¸ì¥ ì´ìƒìœ¼ë¡œ ê° ë¦¬ìŠ¤í¬ë§ˆë‹¤ ëª…í™•í•œ í‰ê°€ì™€ ê·¼ê±°ë¥¼ ì œì‹œí•˜ì„¸ìš”.**"""

                    risk_analysis = await asyncio.to_thread(call_ollama, retry_prompt)

                    if ws:
                        await ws.send_json({
                            "status": "completed",
                            "agent": "Risk_Analyzer",
                            "message": "ì¬ë¶„ì„ ì™„ë£Œ"
                        })

                    continue
                else:
                    if hitl_retry_counts[agent_num] >= MAX_HITL_RETRIES:
                        print(f"[DEBUG] Agent 4 ìµœëŒ€ ì¬ì‹œë„ íšŸìˆ˜ ë„ë‹¬")
                        if ws:
                            await ws.send_json({
                                "status": "processing",
                                "agent": "Risk_Analyzer",
                                "message": "ìµœëŒ€ ì¬ì‹œë„ íšŸìˆ˜ ë„ë‹¬, ë‹¤ìŒ ë‹¨ê³„ë¡œ ì§„í–‰í•©ë‹ˆë‹¤"
                            })
                    break

            if ws:
                next_message = (
                    "ì‚¬ìš©ì ê±´ë„ˆë›°ê¸° ìš”ì²­ì„ ìˆ˜ë½í–ˆìŠµë‹ˆë‹¤. ë‹¤ìŒ ë‹¨ê³„ë¡œ ì§„í–‰í•©ë‹ˆë‹¤."
                    if skip_accepted_agent4
                    else "í”¼ë“œë°± ë°˜ì˜í•˜ì—¬ ë¶„ì„ ê³„ì† ì§„í–‰..."
                )
                await ws.send_json({
                    "status": "processing",
                    "agent": "ROI_Estimator",
                    "message": next_message
                })
            await asyncio.sleep(1)

        # Agent 5: ROI Estimator
        if ws:
            await ws.send_json({"status": "processing", "agent": "ROI_Estimator", "message": "ROI ì¶”ì • ì¤‘..."})

        roi_prompt = f"""ë‹¹ì‹ ì€ AI í”„ë¡œì íŠ¸ì˜ ROI(íˆ¬ì ìˆ˜ìµë¥ ) ë¶„ì„ ì „ë¬¸ê°€ì…ë‹ˆë‹¤.
ë‹¤ìŒ ì œì•ˆì„œì— ëŒ€í•œ ROIë¥¼ ì¶”ì •í•´ì£¼ì„¸ìš”:

ì œì•ˆì„œ ë‚´ìš©:
{proposal_text}

ë‹¤ìŒ í•­ëª©ì„ í‰ê°€í•˜ê³  ì§§ê²Œ ìš”ì•½í•´ì£¼ì„¸ìš”:
1. ì˜ˆìƒ íš¨ê³¼ (ë¹„ìš© ì ˆê°, ìƒì‚°ì„± í–¥ìƒ ë“±)
2. íˆ¬ì ëŒ€ë¹„ íš¨ê³¼ (ROI í¼ì„¼í‹°ì§€, ì†ìµë¶„ê¸°ì )

ê°„ê²°í•˜ê²Œ 2-3ë¬¸ì¥ìœ¼ë¡œ í‰ê°€ ê²°ê³¼ë¥¼ ì‘ì„±í•´ì£¼ì„¸ìš”."""

        roi_estimation = await asyncio.to_thread(call_ollama, roi_prompt)

        if ws:
            await ws.send_json({"status": "completed", "agent": "ROI_Estimator", "message": "ROI ì¶”ì • ì™„ë£Œ"})

        persist_job_metadata(
            job_id,
            "roi_done",
            agent_updates={"roi_estimation": roi_estimation},
        )

        # HITL ì¸í„°ëŸ½íŠ¸: Agent 5 ì´í›„ (ì„¤ì •ì— ë”°ë¼)
        if 5 in hitl_stages:
            agent_num = 5
            skip_accepted_agent5 = False
            while True:
                quality_check = await asyncio.to_thread(
                    analyze_result_quality,
                    "ROI Estimator",
                    roi_estimation,
                    proposal_text
                )
                print(f"[DEBUG] Quality check for Agent 5: {quality_check}")

                feedback_suggestion = await asyncio.to_thread(
                    generate_feedback_suggestion,
                    "ROI Estimator",
                    roi_estimation,
                    proposal_text
                )

                if ws:
                    await ws.send_json({
                        "status": "interrupt",
                        "job_id": job_id,
                        "message": f"ROI ì¶”ì • ê²°ê³¼ í™•ì¸ ì¤‘... (ì¬ì‹œë„ {hitl_retry_counts[agent_num]}/{MAX_HITL_RETRIES}) - {quality_check.get('reason', '')}",
                        "results": {
                            "roi_estimation": roi_estimation,
                            "feedback_suggestion": feedback_suggestion,
                            "quality_check": quality_check
                        }
                    })

                # ì‚¬ìš©ìê°€ ê²°ê³¼ë¥¼ í™•ì¸í•  ë•Œê¹Œì§€ ëŒ€ê¸°
                await wait_for_feedback(job_id)

                updated_job = get_job(job_id)
                skip_requested = updated_job.get("feedback_skip", False)
                user_feedback = (updated_job.get("feedback") or "").strip()

                print(f"[DEBUG] User feedback retrieved (Agent 5): '{user_feedback}' (skip={skip_requested})")

                if skip_requested:
                    skip_accepted_agent5 = True
                    retry_decision = {"needs_retry": False, "reason": "ì‚¬ìš©ìê°€ ê±´ë„ˆë›°ê¸°ë¥¼ ì„ íƒ"}
                    reset_feedback_state(job_id)
                    print(f"[DEBUG] HITL skip acknowledged for Agent 5 (job {job_id})")
                elif user_feedback:
                    retry_decision = {"needs_retry": False, "reason": "ì‚¬ìš©ì í”¼ë“œë°± ë°˜ì˜"}
                    reset_feedback_state(job_id)
                else:
                    retry_decision = quality_check

                print(f"[DEBUG] Retry decision for Agent 5: {retry_decision}")

                if skip_accepted_agent5:
                    break

                if retry_decision.get("needs_retry") and hitl_retry_counts[agent_num] < MAX_HITL_RETRIES:
                    hitl_retry_counts[agent_num] += 1
                    print(f"[DEBUG] Agent 5 ì¬ì‹œë„ {hitl_retry_counts[agent_num]}/{MAX_HITL_RETRIES}")

                    if ws:
                        await ws.send_json({
                            "status": "processing",
                            "agent": "ROI_Estimator",
                            "message": f"í’ˆì§ˆ ê°œì„ ì„ ìœ„í•´ ì¬ê²€í†  ì¤‘... ({hitl_retry_counts[agent_num]}/{MAX_HITL_RETRIES})"
                        })

                    retry_prompt = f"""ë‹¹ì‹ ì€ AI í”„ë¡œì íŠ¸ì˜ ROI(íˆ¬ì ìˆ˜ìµë¥ ) ë¶„ì„ ì „ë¬¸ê°€ì…ë‹ˆë‹¤.
ì´ì „ ë¶„ì„ ê²°ê³¼ê°€ í’ˆì§ˆ ê¸°ì¤€ì„ ì¶©ì¡±í•˜ì§€ ëª»í–ˆìŠµë‹ˆë‹¤. ë” ìƒì„¸í•˜ê³  êµ¬ì²´ì ìœ¼ë¡œ ROIë¥¼ ì¬ì¶”ì •í•´ì£¼ì„¸ìš”.

ì œì•ˆì„œ ë‚´ìš©:
{proposal_text}

ì´ì „ ë¶„ì„ ê²°ê³¼ (ë¶ˆì¶©ë¶„):
{roi_estimation}

í’ˆì§ˆ ê²€ì‚¬ ê²°ê³¼:
- ë¬¸ì œì : {retry_decision.get('reason', 'ë¶„ì„ì´ ë¶ˆì¶©ë¶„í•¨')}
- ë³´ì™„ í•„ìš” ì‚¬í•­: {', '.join(retry_decision.get('additional_info_needed', ['ë” ìƒì„¸í•œ ë¶„ì„']))}

ìœ„ ë¬¸ì œì ì„ í•´ê²°í•˜ê³  ë‹¤ìŒ í•­ëª©ì„ **êµ¬ì²´ì ì´ê³  ìƒì„¸í•˜ê²Œ** ì¬í‰ê°€í•´ì£¼ì„¸ìš”:
1. ì˜ˆìƒ íš¨ê³¼ (êµ¬ì²´ì ì¸ ìˆ˜ì¹˜ì™€ ê·¼ê±°)
2. íˆ¬ì ëŒ€ë¹„ íš¨ê³¼ (ëª…í™•í•œ ROI ê³„ì‚° ê·¼ê±°)

**ë°˜ë“œì‹œ 5-7ë¬¸ì¥ ì´ìƒìœ¼ë¡œ ìˆ˜ì¹˜ì™€ ê³„ì‚° ê·¼ê±°ë¥¼ í¬í•¨í•˜ì—¬ ì‘ì„±í•´ì£¼ì„¸ìš”.**"""

                    roi_estimation = await asyncio.to_thread(call_ollama, retry_prompt)

                    if ws:
                        await ws.send_json({
                            "status": "completed",
                            "agent": "ROI_Estimator",
                            "message": "ì¬ì¶”ì • ì™„ë£Œ"
                        })

                    continue
                else:
                    if hitl_retry_counts[agent_num] >= MAX_HITL_RETRIES:
                        print(f"[DEBUG] Agent 5 ìµœëŒ€ ì¬ì‹œë„ íšŸìˆ˜ ë„ë‹¬")
                        if ws:
                            await ws.send_json({
                                "status": "processing",
                                "agent": "ROI_Estimator",
                                "message": "ìµœëŒ€ ì¬ì‹œë„ íšŸìˆ˜ ë„ë‹¬, ë‹¤ìŒ ë‹¨ê³„ë¡œ ì§„í–‰í•©ë‹ˆë‹¤"
                            })
                    break

            if ws:
                next_message = (
                    "ì‚¬ìš©ì ê±´ë„ˆë›°ê¸° ìš”ì²­ì„ ìˆ˜ë½í–ˆìŠµë‹ˆë‹¤. ë‹¤ìŒ ë‹¨ê³„ë¡œ ì§„í–‰í•©ë‹ˆë‹¤."
                    if skip_accepted_agent5
                    else "í”¼ë“œë°± ë°˜ì˜í•˜ì—¬ ìµœì¢… ë³´ê³ ì„œ ìƒì„± ì¤‘..."
                )
                await ws.send_json({
                    "status": "processing",
                    "agent": "Final_Generator",
                    "message": next_message
                })
            await asyncio.sleep(1)

        # Agent 6: Final Generator
        if ws:
            await ws.send_json({"status": "processing", "agent": "Final_Generator", "message": "ìµœì¢… ë³´ê³ ì„œ ìƒì„± ì¤‘..."})

        final_prompt = f"""ë‹¹ì‹ ì€ AI í”„ë¡œì íŠ¸ ê²€í†  ì „ë¬¸ê°€ì…ë‹ˆë‹¤.
ë‹¤ìŒ ì œì•ˆì„œì™€ ë¶„ì„ ê²°ê³¼ë¥¼ ë°”íƒ•ìœ¼ë¡œ ìµœì¢… ì˜ê²¬ì„ ì‘ì„±í•´ì£¼ì„¸ìš”:

ì œì•ˆì„œ ë‚´ìš©:
{proposal_text}

ëª©í‘œ ê²€í† :
{objective_review}

ë°ì´í„° ë¶„ì„:
{data_analysis}

ë¦¬ìŠ¤í¬ ë¶„ì„:
{risk_analysis}

ROI ì¶”ì •:
{roi_estimation}

ë‹¤ìŒì„ í¬í•¨í•œ ìµœì¢… ì˜ê²¬ì„ ì‘ì„±í•´ì£¼ì„¸ìš”:
1. ìŠ¹ì¸ ë˜ëŠ” ë³´ë¥˜ ê¶Œì¥ (ëª…í™•í•˜ê²Œ)
2. ì£¼ìš” ê·¼ê±° (3-4ê°€ì§€)
3. ê¶Œì¥ì‚¬í•­ (2-3ê°€ì§€)

ê°„ê²°í•˜ê²Œ 5-7ë¬¸ì¥ìœ¼ë¡œ ì‘ì„±í•´ì£¼ì„¸ìš”."""

        final_recommendation = await asyncio.to_thread(call_ollama, final_prompt)

        if ws:
            await ws.send_json({"status": "completed", "agent": "Final_Generator", "message": "ìµœì¢… ì˜ê²¬ ìƒì„± ì™„ë£Œ"})
        update_job_status(job_id, "final_done")

        # HITL ì¸í„°ëŸ½íŠ¸: Agent 6 ì´í›„ (ì„¤ì •ì— ë”°ë¼)
        if 6 in hitl_stages:
            agent_num = 6
            skip_accepted_agent6 = False
            while True:
                quality_check = await asyncio.to_thread(
                    analyze_result_quality,
                    "Final Generator",
                    final_recommendation,
                    proposal_text
                )
                print(f"[DEBUG] Quality check for Agent 6: {quality_check}")

                feedback_suggestion = await asyncio.to_thread(
                    generate_feedback_suggestion,
                    "Final Generator",
                    final_recommendation,
                    proposal_text
                )

                if ws:
                    await ws.send_json({
                        "status": "interrupt",
                        "job_id": job_id,
                        "message": f"ìµœì¢… ì˜ê²¬ í™•ì¸ ì¤‘... (ì¬ì‹œë„ {hitl_retry_counts[agent_num]}/{MAX_HITL_RETRIES}) - {quality_check.get('reason', '')}",
                        "results": {
                            "final_recommendation": final_recommendation,
                            "feedback_suggestion": feedback_suggestion,
                            "quality_check": quality_check
                        }
                    })

                # ì‚¬ìš©ìê°€ ê²°ê³¼ë¥¼ í™•ì¸í•  ë•Œê¹Œì§€ ëŒ€ê¸°
                await wait_for_feedback(job_id)

                updated_job = get_job(job_id)
                skip_requested = updated_job.get("feedback_skip", False)
                user_feedback = (updated_job.get("feedback") or "").strip()

                print(f"[DEBUG] User feedback retrieved (Agent 6): '{user_feedback}' (skip={skip_requested})")

                if skip_requested:
                    skip_accepted_agent6 = True
                    retry_decision = {"needs_retry": False, "reason": "ì‚¬ìš©ìê°€ ê±´ë„ˆë›°ê¸°ë¥¼ ì„ íƒ"}
                    reset_feedback_state(job_id)
                    print(f"[DEBUG] HITL skip acknowledged for Agent 6 (job {job_id})")
                elif user_feedback:
                    retry_decision = {"needs_retry": False, "reason": "ì‚¬ìš©ì í”¼ë“œë°± ë°˜ì˜"}
                    reset_feedback_state(job_id)
                else:
                    retry_decision = quality_check

                print(f"[DEBUG] Retry decision for Agent 6: {retry_decision}")

                if skip_accepted_agent6:
                    break

                if retry_decision.get("needs_retry") and hitl_retry_counts[agent_num] < MAX_HITL_RETRIES:
                    hitl_retry_counts[agent_num] += 1
                    print(f"[DEBUG] Agent 6 ì¬ì‹œë„ {hitl_retry_counts[agent_num]}/{MAX_HITL_RETRIES}")

                    if ws:
                        await ws.send_json({
                            "status": "processing",
                            "agent": "Final_Generator",
                            "message": f"í’ˆì§ˆ ê°œì„ ì„ ìœ„í•´ ì¬ê²€í†  ì¤‘... ({hitl_retry_counts[agent_num]}/{MAX_HITL_RETRIES})"
                        })

                    retry_prompt = f"""ë‹¹ì‹ ì€ AI í”„ë¡œì íŠ¸ ê²€í†  ì „ë¬¸ê°€ì…ë‹ˆë‹¤.
ì´ì „ ìµœì¢… ì˜ê²¬ì´ í’ˆì§ˆ ê¸°ì¤€ì„ ì¶©ì¡±í•˜ì§€ ëª»í–ˆìŠµë‹ˆë‹¤. ë” ìƒì„¸í•˜ê³  êµ¬ì²´ì ìœ¼ë¡œ ìµœì¢… ì˜ê²¬ì„ ì¬ì‘ì„±í•´ì£¼ì„¸ìš”.

ì œì•ˆì„œ ë‚´ìš©:
{proposal_text}

ëª©í‘œ ê²€í† :
{objective_review}

ë°ì´í„° ë¶„ì„:
{data_analysis}

ë¦¬ìŠ¤í¬ ë¶„ì„:
{risk_analysis}

ROI ì¶”ì •:
{roi_estimation}

ì´ì „ ìµœì¢… ì˜ê²¬ (ë¶ˆì¶©ë¶„):
{final_recommendation}

í’ˆì§ˆ ê²€ì‚¬ ê²°ê³¼:
- ë¬¸ì œì : {retry_decision.get('reason', 'ì˜ê²¬ì´ ë¶ˆì¶©ë¶„í•¨')}
- ë³´ì™„ í•„ìš” ì‚¬í•­: {', '.join(retry_decision.get('additional_info_needed', ['ë” ëª…í™•í•œ íŒë‹¨', 'êµ¬ì²´ì ì¸ ê·¼ê±°']))}

ìœ„ ë¬¸ì œì ì„ í•´ê²°í•˜ê³  ë‹¤ìŒì„ í¬í•¨í•œ **êµ¬ì²´ì ì´ê³  ëª…í™•í•œ** ìµœì¢… ì˜ê²¬ì„ ì‘ì„±í•´ì£¼ì„¸ìš”:
1. ìŠ¹ì¸/ë³´ë¥˜ ê¶Œì¥ (ëª…í™•í•œ ê²°ì •ê³¼ ì´ìœ )
2. ì£¼ìš” ê·¼ê±° (êµ¬ì²´ì ì¸ ë°ì´í„°ì™€ ë¶„ì„ ê²°ê³¼ ì¸ìš©)
3. ì‹¤í–‰ ê¶Œì¥ì‚¬í•­ (êµ¬ì²´ì ì´ê³  ì‹¤í˜„ ê°€ëŠ¥í•œ ì œì•ˆ)

**ë°˜ë“œì‹œ 7-10ë¬¸ì¥ ì´ìƒìœ¼ë¡œ ëª…í™•í•œ íŒë‹¨ê³¼ ìƒì„¸í•œ ê·¼ê±°ë¥¼ í¬í•¨í•˜ì—¬ ì‘ì„±í•´ì£¼ì„¸ìš”.**"""

                    final_recommendation = await asyncio.to_thread(call_ollama, retry_prompt)

                    if ws:
                        await ws.send_json({
                            "status": "completed",
                            "agent": "Final_Generator",
                            "message": "ì¬ê²€í†  ì™„ë£Œ"
                        })

                    continue
                else:
                    if hitl_retry_counts[agent_num] >= MAX_HITL_RETRIES:
                        print(f"[DEBUG] Agent 6 ìµœëŒ€ ì¬ì‹œë„ íšŸìˆ˜ ë„ë‹¬")
                        if ws:
                            await ws.send_json({
                                "status": "processing",
                                "agent": "Final_Generator",
                                "message": "ìµœëŒ€ ì¬ì‹œë„ íšŸìˆ˜ ë„ë‹¬, ìµœì¢… ë³´ê³ ì„œë¥¼ ìƒì„±í•©ë‹ˆë‹¤"
                            })
                    break

            if ws:
                next_message = (
                    "ì‚¬ìš©ì ê±´ë„ˆë›°ê¸° ìš”ì²­ì„ ìˆ˜ë½í–ˆìŠµë‹ˆë‹¤. ìµœì¢… ë³´ê³ ì„œë¥¼ ìƒì„±í•©ë‹ˆë‹¤."
                    if skip_accepted_agent6
                    else "í”¼ë“œë°± ë°˜ì˜í•˜ì—¬ ìµœì¢… ë³´ê³ ì„œ ìƒì„± ì¤‘..."
                )
                await ws.send_json({"status": "processing", "message": next_message})
            await asyncio.sleep(1)

        # ìµœì¢… ì™„ë£Œ
        final_report = f"""
        <div style="padding: 20px;">
            <h2>ğŸ“Š AI ê³¼ì œ ì§€ì›ì„œ ê²€í†  ë³´ê³ ì„œ</h2>
            <hr/>

            <div class="accordion-item">
                <div class="accordion-header" onclick="toggleAccordion('section1')">
                    <span>1. BP ì‚¬ë¡€ ë¶„ì„ ({len(bp_cases)}ê±´)</span>
                    <span class="accordion-icon">â–¼</span>
                </div>
                <div id="section1" class="accordion-content" style="display: block;">
                    <p><strong>ìœ ì‚¬ ì‚¬ë¡€:</strong></p>
                    <ul>
                        {''.join([f'<li>{c.get("title", "")} ({c.get("domain", "")}/{c.get("division", "")})</li>' for c in bp_cases]) if bp_cases else '<li>ê²€ìƒ‰ëœ ì‚¬ë¡€ ì—†ìŒ</li>'}
                    </ul>
                    <p><em>ì´ {len(bp_cases)}ê±´ì˜ ìœ ì‚¬ ì‚¬ë¡€ê°€ ë°œê²¬ë˜ì—ˆìŠµë‹ˆë‹¤.</em></p>
                </div>
            </div>

            <div class="accordion-item">
                <div class="accordion-header" onclick="toggleAccordion('section2')">
                    <span>2. ëª©í‘œ ì í•©ì„±</span>
                    <span class="accordion-icon">â–¼</span>
                </div>
                <div id="section2" class="accordion-content">
                    <p>{objective_review.replace(chr(10), '<br>')}</p>
                </div>
            </div>

            <div class="accordion-item">
                <div class="accordion-header" onclick="toggleAccordion('section3')">
                    <span>3. ë°ì´í„° ë¶„ì„</span>
                    <span class="accordion-icon">â–¼</span>
                </div>
                <div id="section3" class="accordion-content">
                    <p>{data_analysis.replace(chr(10), '<br>')}</p>
                </div>
            </div>

            <div class="accordion-item">
                <div class="accordion-header" onclick="toggleAccordion('section4')">
                    <span>4. ë¦¬ìŠ¤í¬ ë¶„ì„</span>
                    <span class="accordion-icon">â–¼</span>
                </div>
                <div id="section4" class="accordion-content">
                    <p>{risk_analysis.replace(chr(10), '<br>')}</p>
                </div>
            </div>

            <div class="accordion-item">
                <div class="accordion-header" onclick="toggleAccordion('section5')">
                    <span>5. ROI ì¶”ì •</span>
                    <span class="accordion-icon">â–¼</span>
                </div>
                <div id="section5" class="accordion-content">
                    <p>{roi_estimation.replace(chr(10), '<br>')}</p>
                </div>
            </div>

            <div class="accordion-item">
                <div class="accordion-header" onclick="toggleAccordion('section6')">
                    <span>6. ìµœì¢… ì˜ê²¬</span>
                    <span class="accordion-icon">â–¼</span>
                </div>
                <div id="section6" class="accordion-content" style="display: block;">
                    <p>{final_recommendation.replace(chr(10), '<br>')}</p>
                </div>
            </div>
        </div>
        """

        decision_result = await classify_final_decision(final_report, final_recommendation)
        decision_value = decision_result.get("decision", "ë³´ë¥˜")
        decision_reason = decision_result.get("reason", "LLM íŒë‹¨ì„ ê¸°ì¤€ìœ¼ë¡œ ìë™ ë¶„ë¥˜ë˜ì—ˆìŠµë‹ˆë‹¤.")

        latest_job = get_job(job_id) or {}
        metadata = latest_job.get("metadata", {}).copy()
        metadata["report"] = final_report
        agent_results = metadata.setdefault("agent_results", {})
        agent_results["final_recommendation"] = final_recommendation
        metadata["final_decision"] = {
            "decision": decision_value,
            "reason": decision_reason,
        }

        update_job_status(
            job_id,
            "completed",
            metadata=metadata,
            llm_decision=decision_value,
        )

        if send_final_report:
            target_ws = ws or active_connections.get(ws_key)
            if target_ws:
                human_decision_value = latest_job.get("decision") or latest_job.get("human_decision")
                await target_ws.send_json({
                    "status": "completed",
                    "agent": "Final_Generator",
                    "message": "ê²€í†  ì™„ë£Œ",
                    "report": final_report,
                    "decision": decision_value,
                    "decision_reason": decision_reason,
                    "human_decision": human_decision_value,
                })

    except Exception as e:
        print(f"!!! ERROR in review process: {e}")
        import traceback
        traceback.print_exc()
        if ws:
            try:
                await ws.send_json({"status": "error", "message": f"Error: {str(e)}"})
            except:
                pass
        update_job_status(job_id, "error")
    finally:
        print(f"=== process_review EXIT for job {job_id} ===")

@app.websocket("/ws/{job_id}")
async def websocket_endpoint(websocket: WebSocket, job_id: str):
    """WebSocket ì‹¤ì‹œê°„ ì§„í–‰ìƒí™© ì—…ë°ì´íŠ¸"""
    await websocket.accept()
    active_connections[job_id] = websocket

    try:
        while True:
            # í´ë¼ì´ì–¸íŠ¸ë¡œë¶€í„° ë©”ì‹œì§€ ìˆ˜ì‹  (keep-alive)
            data = await websocket.receive_text()
            # ì—ì½”ë°± (ì—°ê²° ìœ ì§€)
            await websocket.send_json({"type": "pong"})
    except WebSocketDisconnect:
        print(f"WebSocket ì—°ê²° ì¢…ë£Œ: {job_id}")
        del active_connections[job_id]
    except Exception as e:
        print(f"WebSocket ì—ëŸ¬: {e}")
        if job_id in active_connections:
            del active_connections[job_id]

@app.post("/api/v1/review/feedback/{job_id}")
async def submit_feedback(job_id: int, feedback: dict):
    """HITL í”¼ë“œë°± ì œì¶œ"""
    print(f"[DEBUG] Feedback received (Job {job_id}): {feedback}")

    # í”¼ë“œë°± í…ìŠ¤íŠ¸ ì¶”ì¶œ
    feedback_text = feedback.get("feedback", "") or ""
    if isinstance(feedback_text, str):
        feedback_text = feedback_text.strip()
    else:
        feedback_text = str(feedback_text)

    skip_requested = bool(feedback.get("skip"))
    print(f"[DEBUG] Feedback text: {feedback_text}")
    print(f"[DEBUG] Skip requested: {skip_requested}")

    # í”¼ë“œë°±ì„ jobì˜ metadataì— ì €ì¥
    update_job_feedback(job_id, feedback_text, skip=skip_requested)

    # DB ìƒíƒœë¥¼ feedback_receivedë¡œ ì—…ë°ì´íŠ¸
    update_job_status(job_id, "feedback_received")

    print(f"[DEBUG] Feedback saved and status updated for job {job_id}")

    return {"status": "feedback_received", "job_id": job_id, "skip": skip_requested}

@app.get("/api/v1/review/pdf/{job_id}")
async def download_pdf(job_id: int):
    """PDF ë‹¤ìš´ë¡œë“œ"""
    # MVP: ê°„ë‹¨í•œ ì‘ë‹µ
    return {"message": "PDF ìƒì„± ê¸°ëŠ¥ì€ ì¶”í›„ êµ¬í˜„ ì˜ˆì •", "job_id": job_id}

# ==================== Confluence API ì—”ë“œí¬ì¸íŠ¸ ====================

@app.post("/api/v1/confluence/fetch-pages")
async def fetch_confluence_pages(
    page_id: str = Form(...),
    include_children: bool = Form(True),
    include_current: bool = Form(True),
    max_depth: int = Form(2)
):
    """
    Confluence í˜ì´ì§€ ê°€ì ¸ì˜¤ê¸°
    - page_id: Confluence í˜ì´ì§€ ID
    - include_children: í•˜ìœ„ í˜ì´ì§€ í¬í•¨ ì—¬ë¶€
    - include_current: í˜„ì¬ í˜ì´ì§€ í¬í•¨ ì—¬ë¶€
    - max_depth: í•˜ìœ„ í˜ì´ì§€ íƒìƒ‰ ê¹Šì´ (1-5)
    """
    try:
        if not page_id:
            return JSONResponse(
                status_code=400,
                content={"error": "page_idê°€ í•„ìš”í•©ë‹ˆë‹¤"}
            )

        # ê¹Šì´ ì œí•œ
        max_depth = max(1, min(max_depth, 5))

        if include_children:
            # ì¬ê·€ì ìœ¼ë¡œ í˜ì´ì§€ ê°€ì ¸ì˜¤ê¸°
            pages = await asyncio.to_thread(
                get_pages_recursively,
                page_id,
                include_current=include_current,
                max_depth=max_depth,
                current_depth=0
            )
        else:
            # í˜„ì¬ í˜ì´ì§€ë§Œ ê°€ì ¸ì˜¤ê¸°
            page = await asyncio.to_thread(get_page_content, page_id)
            pages = [page] if page else []

        if not pages:
            return JSONResponse(
                status_code=404,
                content={"error": "í˜ì´ì§€ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤"}
            )

        # í˜ì´ì§€ ì •ë³´ ìš”ì•½
        page_summaries = [
            {
                "id": p.get("id"),
                "title": p.get("title"),
                "content_length": len(p.get("content", "")),
                "space": p.get("space")
            }
            for p in pages
        ]

        return {
            "status": "success",
            "page_count": len(pages),
            "pages": page_summaries,
            "combined_content_length": sum(len(p.get("content", "")) for p in pages)
        }

    except Exception as e:
        print(f"Confluence í˜ì´ì§€ ê°€ì ¸ì˜¤ê¸° ì‹¤íŒ¨: {str(e)}")
        return JSONResponse(
            status_code=500,
            content={"error": f"í˜ì´ì§€ ê°€ì ¸ì˜¤ê¸° ì‹¤íŒ¨: {str(e)}"}
        )

@app.post("/api/v1/confluence/submit-for-review")
async def submit_confluence_for_review(
    page_id: str = Form(...),
    include_children: bool = Form(True),
    include_current: bool = Form(True),
    max_depth: int = Form(2),
    domain: str = Form("ì œì¡°"),
    division: str = Form("ë©”ëª¨ë¦¬"),
    hitl_stages: str = Form("[]")
):
    """
    Confluence í˜ì´ì§€ë¥¼ ê°€ì ¸ì™€ì„œ ê²€í†  ì‹œì‘
    """
    try:
        # ê¹Šì´ ì œí•œ
        max_depth = max(1, min(max_depth, 5))

        # í˜ì´ì§€ ê°€ì ¸ì˜¤ê¸°
        if include_children:
            pages = await asyncio.to_thread(
                get_pages_recursively,
                page_id,
                include_current=include_current,
                max_depth=max_depth,
                current_depth=0
            )
        else:
            page = await asyncio.to_thread(get_page_content, page_id)
            pages = [page] if page else []

        if not pages:
            return JSONResponse(
                status_code=404,
                content={"error": "í˜ì´ì§€ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤"}
            )

        # HITL ë‹¨ê³„ íŒŒì‹±
        try:
            hitl_stages_list = json.loads(hitl_stages)
        except:
            hitl_stages_list = []

        # ê° í˜ì´ì§€ë³„ë¡œ job ìƒì„± ë° ìˆœì°¨ ì²˜ë¦¬
        job_ids = []
        page_list = [{"id": p.get("id"), "title": p.get("title") or ""} for p in pages]

        for idx, page in enumerate(pages):
            raw_title = page.get('title') or ''
            page_content = f"{'='*80}\ní˜ì´ì§€: {raw_title}\nID: {page.get('id')}\n{'='*80}\n{page.get('content')}"
            job_title = raw_title.strip() or await generate_job_title(page_content, fallback=f"Confluence í˜ì´ì§€ {idx+1}")
            job_id = create_job(
                page_content,
                domain,
                division,
                title=job_title,
                hitl_stages=hitl_stages_list,
            )
            page_list[idx]["title"] = job_title
            job_ids.append(job_id)
            print(f"Created job {job_id} for page {idx+1}/{len(pages)}: {job_title}")

        # ì²« ë²ˆì§¸ í˜ì´ì§€ë¶€í„° ìˆœì°¨ì ìœ¼ë¡œ ì²˜ë¦¬ ì‹œì‘
        print(f"Starting sequential processing for {len(job_ids)} pages")
        asyncio.create_task(process_confluence_pages_sequentially(job_ids, page_list))

        return {
            "status": "submitted",
            "job_id": job_ids[0],  # ì²« ë²ˆì§¸ job_idë¥¼ ë©”ì¸ìœ¼ë¡œ ì‚¬ìš©
            "job_ids": job_ids,
            "page_count": len(pages),
            "pages": page_list
        }

    except Exception as e:
        print(f"Confluence ê²€í†  ì œì¶œ ì‹¤íŒ¨: {str(e)}")
        return JSONResponse(
            status_code=500,
            content={"error": f"ê²€í†  ì œì¶œ ì‹¤íŒ¨: {str(e)}"}
        )

@app.get("/api/v1/confluence/child-pages/{page_id}")
async def get_confluence_child_pages(page_id: str):
    """íŠ¹ì • í˜ì´ì§€ì˜ í•˜ìœ„ í˜ì´ì§€ ëª©ë¡ ì¡°íšŒ"""
    try:
        children = await asyncio.to_thread(get_child_pages, page_id)

        return {
            "status": "success",
            "page_id": page_id,
            "child_count": len(children),
            "children": children
        }

    except Exception as e:
        print(f"í•˜ìœ„ í˜ì´ì§€ ì¡°íšŒ ì‹¤íŒ¨: {str(e)}")
        return JSONResponse(
            status_code=500,
            content={"error": f"í•˜ìœ„ í˜ì´ì§€ ì¡°íšŒ ì‹¤íŒ¨: {str(e)}"}
        )


# ==================== Dashboard & CRUD API ====================


@app.get("/dashboard")
async def dashboard_page():
    """ëŒ€ì‹œë³´ë“œ HTML ì œê³µ"""
    return FileResponse("static/dashboard.html")


def _sanitize_decision(decision: str | None) -> str | None:
    if not decision:
        return decision
    normalized = decision.strip()
    return normalized


def _coerce_hitl_stages(values: Optional[List[int] | List[str]]) -> Optional[List[int]]:
    if values is None:
        return None
    coerced = []
    for item in values:
        try:
            coerced.append(int(item))
        except (TypeError, ValueError):
            continue
    return coerced


@app.get("/api/v1/dashboard/jobs")
async def dashboard_list(
    status: Optional[str] = None,
    decision: Optional[str] = None,
    llm_decision: Optional[str] = None,
    search: Optional[str] = None,
    limit: int = 50,
    offset: int = 0,
    order: str = "desc",
):
    limit = max(1, min(limit, 200))
    offset = max(0, offset)

    jobs = list_jobs(
        limit=limit,
        offset=offset,
        status=status,
        decision=decision,
        llm_decision=llm_decision,
        search=search,
        order=order,
    )

    total = count_jobs(status=status, decision=decision, llm_decision=llm_decision, search=search)

    formatted_jobs = []
    for job in jobs:
        job_copy = job.copy()
        proposal_text = (job_copy.get("proposal_content") or "")
        job_copy["proposal_preview"] = proposal_text[:200]
        formatted_jobs.append(job_copy)

    return {
        "total": total,
        "limit": limit,
        "offset": offset,
        "jobs": formatted_jobs,
    }


@app.get("/api/v1/dashboard/jobs/{job_id}")
async def dashboard_get_job_detail(job_id: int):
    job = get_job(job_id)
    if not job:
        raise HTTPException(status_code=404, detail="ì¡´ì¬í•˜ì§€ ì•ŠëŠ” ì‘ì—…ì…ë‹ˆë‹¤.")
    return job


@app.post("/api/v1/dashboard/jobs")
async def dashboard_create_job(payload: JobCreateRequest):
    metadata_payload = payload.metadata.copy() if payload.metadata else {}
    hitl_stages = payload.hitl_stages
    if hitl_stages is None and "hitl_stages" in metadata_payload:
        hitl_stages = _coerce_hitl_stages(metadata_payload.pop("hitl_stages"))
    else:
        hitl_stages = _coerce_hitl_stages(hitl_stages)

    human_decision_value = _sanitize_decision(payload.human_decision) or "pending"
    llm_decision_value = _sanitize_decision(payload.llm_decision) if payload.llm_decision else "pending"
    title_value = (payload.title or "").strip()
    if not title_value:
        title_value = await generate_job_title(payload.proposal_content, fallback=f"{payload.domain} ì œì•ˆì„œ")

    job_id = create_job(
        payload.proposal_content,
        payload.domain,
        payload.division,
        title=title_value,
        status=payload.status,
        human_decision=human_decision_value,
        llm_decision=llm_decision_value,
        metadata=metadata_payload,
        hitl_stages=hitl_stages,
    )

    created_job = get_job(job_id)
    return created_job


@app.put("/api/v1/dashboard/jobs/{job_id}")
async def dashboard_update_job(job_id: int, payload: JobUpdateRequest):
    existing = get_job(job_id)
    if not existing:
        raise HTTPException(status_code=404, detail="ì¡´ì¬í•˜ì§€ ì•ŠëŠ” ì‘ì—…ì…ë‹ˆë‹¤.")

    metadata_payload = None
    if payload.metadata is not None:
        base_metadata = existing.get("metadata", {}).copy()
        if payload.metadata == {}:
            base_metadata = {}
        else:
            base_metadata.update(payload.metadata)
        metadata_payload = base_metadata

    if payload.hitl_stages is not None:
        stages = _coerce_hitl_stages(payload.hitl_stages)
        if metadata_payload is None:
            metadata_payload = existing.get("metadata", {}).copy()
        metadata_payload["hitl_stages"] = stages or []

    success = update_job_record(
        job_id,
        title=payload.title,
        proposal_content=payload.proposal_content,
        domain=payload.domain,
        division=payload.division,
        status=payload.status,
        human_decision=_sanitize_decision(payload.human_decision) if payload.human_decision is not None else None,
        llm_decision=_sanitize_decision(payload.llm_decision) if payload.llm_decision is not None else None,
        metadata=metadata_payload,
    )

    if not success:
        raise HTTPException(status_code=400, detail="ì—…ë°ì´íŠ¸í•  í•„ë“œê°€ ì—†ìŠµë‹ˆë‹¤.")

    return get_job(job_id)


@app.delete("/api/v1/dashboard/jobs/{job_id}")
async def dashboard_delete_job(job_id: int):
    job = get_job(job_id)
    if not job:
        raise HTTPException(status_code=404, detail="ì¡´ì¬í•˜ì§€ ì•ŠëŠ” ì‘ì—…ì…ë‹ˆë‹¤.")

    delete_job(job_id)
    return {"status": "deleted", "job_id": job_id}


if __name__ == "__main__":
    print(f"Server starting at http://{HOST}:{PORT}")
    uvicorn.run(app, host=HOST, port=PORT)
