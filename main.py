# main.py - FastAPI í†µí•© ì„œë²„ êµ¬í˜„
from fastapi import FastAPI, WebSocket, WebSocketDisconnect
from fastapi.staticfiles import StaticFiles
from fastapi.middleware.cors import CORSMiddleware
import uvicorn
import asyncio
from pathlib import Path
import ollama
import os
import uuid
import json
import re
import requests
from dotenv import load_dotenv

from database.db import (
    init_database,
    create_job,
    get_job,
    list_jobs,
    update_job_status,
    update_job_record,
    update_job_feedback,
    reset_feedback_state,
    delete_job,
    count_jobs,
)
from typing import Optional
from config.settings import HOST, PORT
from confluence_api import get_page_content, get_child_pages, get_pages_recursively, combine_pages_content

# Import agent modules
from agents import (
    run_bp_scouter,
    run_objective_reviewer,
    run_data_analyzer,
    run_risk_analyzer,
    run_roi_estimator,
    run_final_generator,
    run_proposal_improver,
)

# Import API routers
from api.health import router as health_router
from api.review import init_review_router, router as review_router
from api.confluence import init_confluence_router, router as confluence_router
from api.dashboard import init_dashboard_router, router as dashboard_router
from api.pdf_export import init_pdf_export_router, router as pdf_export_router

# í™˜ê²½ ë³€ìˆ˜ ë¡œë“œ
load_dotenv()

app = FastAPI(title="AI Proposal Reviewer", version="1.0.0")

# CORS ì„¤ì • (MVP: ëª¨ë“  origin í—ˆìš©)
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# ì •ì  íŒŒì¼ ì„œë¹™ (í”„ë¡ íŠ¸ì—”ë“œ)
static_path = Path("static")
if static_path.exists():
    app.mount("/static", StaticFiles(directory="static"), name="static")

# WebSocket ì—°ê²° ê´€ë¦¬
active_connections: dict[str, WebSocket] = {}


def _extract_json_dict(text: str) -> Optional[dict]:
    if not text:
        return None
    try:
        parsed = json.loads(text)
        if isinstance(parsed, dict):
            return parsed
    except json.JSONDecodeError:
        pass

    match = re.search(r'\{.*\}', text, re.DOTALL)
    if match:
        try:
            parsed = json.loads(match.group())
            if isinstance(parsed, dict):
                return parsed
        except json.JSONDecodeError:
            return None
    return None


def _truncate_for_prompt(text: str, limit: int = 800) -> str:
    if not text:
        return ''
    text = text.strip()
    if len(text) <= limit:
        return text
    return text[:limit] + '...'


def _generate_title_sync(content: str, fallback: str) -> str:
    prompt_body = _truncate_for_prompt(content, 600)
    prompt = f"""
ë‹¹ì‹ ì€ ì œì•ˆì„œ ì œëª©ì„ ë§Œë“œëŠ” ì „ë¬¸ê°€ì…ë‹ˆë‹¤. ì•„ë˜ ì œì•ˆì„œ ë‚´ìš©ì„ ë³´ê³  í•µì‹¬ì„ í‘œí˜„í•˜ëŠ” 25ì ì´í•˜ì˜ í•œêµ­ì–´ ì œëª©ì„ ì‘ì„±í•˜ì„¸ìš”.
ì œëª©ì€ íŠ¹ìˆ˜ë¬¸ì ì—†ì´ ê°„ê²°í•˜ê²Œ ì‘ì„±í•˜ê³ , JSON í˜•ì‹ìœ¼ë¡œë§Œ ì‘ë‹µí•˜ì„¸ìš”.

ì œì•ˆì„œ:
{prompt_body}

ì‘ë‹µ í˜•ì‹:
{{"title": "ì—¬ê¸°ì— ì œëª©"}}
"""
    response = call_llm(prompt)
    data = _extract_json_dict(response)
    if data and isinstance(data.get('title'), str):
        title = data['title'].strip()
        if title:
            return title[:50]
    snippet_lines = (content or '').strip().splitlines()
    for line in snippet_lines:
        line = line.strip()
        if line:
            return line[:50]
    return fallback


async def generate_job_title(content: str, fallback: str) -> str:
    return await asyncio.to_thread(_generate_title_sync, content, fallback)


@app.on_event("startup")
async def startup_event():
    """ì„œë²„ ì‹œì‘ ì‹œ ë°ì´í„°ë² ì´ìŠ¤ ë° LLM ì´ˆê¸°í™”"""
    print("Server starting...")
    init_database()
    print("Database ready")
    init_llm()
    print("LLM ready")

    # Initialize API routers with dependencies
    init_review_router(
        active_connections=active_connections,
        process_review_func=process_review,
        generate_job_title_func=generate_job_title,
        create_job_func=create_job,
        update_job_feedback_func=update_job_feedback,
        update_job_status_func=update_job_status,
        get_job_func=get_job,
    )

    init_confluence_router(
        active_connections=active_connections,
        process_confluence_pages_sequentially_func=process_confluence_pages_sequentially,
        get_page_content_func=get_page_content,
        get_child_pages_func=get_child_pages,
        get_pages_recursively_func=get_pages_recursively,
        generate_job_title_func=generate_job_title,
        create_job_func=create_job,
    )

    init_dashboard_router(
        list_jobs_func=list_jobs,
        count_jobs_func=count_jobs,
        get_job_func=get_job,
        create_job_func=create_job,
        update_job_record_func=update_job_record,
        delete_job_func=delete_job,
        generate_job_title_func=generate_job_title,
    )

    init_pdf_export_router(
        get_job_func=get_job,
    )


# Register routers
app.include_router(health_router)
app.include_router(review_router)
app.include_router(confluence_router)
app.include_router(dashboard_router)
app.include_router(pdf_export_router)


# LLM ì„¤ì • ë° ì´ˆê¸°í™”
LLM_PROVIDER = os.getenv("LLM_PROVIDER", "ollama")
llm_client = None

def init_llm():
    """í™˜ê²½ ë³€ìˆ˜ì— ë”°ë¼ LLM í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™”"""
    global llm_client

    if LLM_PROVIDER == "internal":
        # Internal LLM ì„¤ì • (lazy import to avoid pydantic version issues)
        from langchain_openai import ChatOpenAI

        base_url = os.getenv("INTERNAL_BASE_URL")
        model = os.getenv("INTERNAL_MODEL")
        credential_key = os.getenv("INTERNAL_CREDENTIAL_KEY")
        system_name = os.getenv("INTERNAL_SYSTEM_NAME")
        user_id = os.getenv("INTERNAL_USER_ID")

        llm_client = ChatOpenAI(
            base_url=base_url,
            model=model,
            default_headers={
                "x-dep-ticket": credential_key,
                "Send-System-Name": system_name,
                "User-ID": user_id,
                "User-Type": "AD",
                "Prompt-Msg-Id": str(uuid.uuid4()),
                "Completion-Msg-Id": str(uuid.uuid4()),
            },
        )
        print(f"Internal LLM initialized: {model}")
    else:
        # Ollama ì„¤ì •
        llm_client = "ollama"  # OllamaëŠ” ì§ì ‘ í•¨ìˆ˜ë¡œ í˜¸ì¶œ
        print(f"Ollama LLM initialized: {os.getenv('OLLAMA_MODEL', 'gemma2:2b')}")

def clean_unicode_for_cp949(text: str) -> str:
    """CP949 ì¸ì½”ë”©ì—ì„œ ë¬¸ì œê°€ ë˜ëŠ” ìœ ë‹ˆì½”ë“œ ë¬¸ìë¥¼ ì•ˆì „í•˜ê²Œ ì œê±°"""
    if not text:
        return text

    # CP949ë¡œ ì¸ì½”ë”© ê°€ëŠ¥í•œ ë¬¸ìë§Œ ìœ ì§€
    try:
        # ë¨¼ì € CP949ë¡œ ì¸ì½”ë”© ì‹œë„
        text.encode('cp949')
        return text
    except UnicodeEncodeError:
        # ì¸ì½”ë”© ì‹¤íŒ¨ ì‹œ ë¬¸ìë³„ë¡œ ì²˜ë¦¬
        cleaned = []
        for char in text:
            try:
                char.encode('cp949')
                cleaned.append(char)
            except UnicodeEncodeError:
                # CP949ë¡œ ì¸ì½”ë”©í•  ìˆ˜ ì—†ëŠ” ë¬¸ìëŠ” ê³µë°± ë˜ëŠ” ? ë¡œ ëŒ€ì²´
                if char.isspace():
                    cleaned.append(' ')
                else:
                    cleaned.append('?')
        return ''.join(cleaned)

def call_llm(prompt: str, enable_sequential_thinking: bool = False, use_context7: bool = False) -> str:
    """í†µí•© LLM í˜¸ì¶œ í•¨ìˆ˜

    Args:
        prompt: LLMì— ì „ë‹¬í•  í”„ë¡¬í”„íŠ¸
        enable_sequential_thinking: Sequential Thinking MCP í™œì„±í™” ì—¬ë¶€
        use_context7: Context7 tool í™œì„±í™” ì—¬ë¶€

    Returns:
        LLM ì‘ë‹µ ë¬¸ìì—´
    """
    try:
        if LLM_PROVIDER == "internal":
            # Internal LLM ì‚¬ìš© (tool calling ì§€ì›)
            if enable_sequential_thinking or use_context7:
                # Tool calling í™œì„±í™”
                from langchain_core.tools import tool

                @tool
                def sequential_thinking(
                    thought: str,
                    next_thought_needed: bool,
                    thought_number: int,
                    total_thoughts: int,
                    is_revision: bool = False,
                    revises_thought: int = None,
                    branch_from_thought: int = None,
                    branch_id: str = None,
                    needs_more_thoughts: bool = False
                ) -> str:
                    """Sequential Thinking tool for step-by-step reasoning.

                    This tool helps analyze problems through a flexible thinking process.
                    Each thought can build on, question, or revise previous insights.

                    Args:
                        thought: Current thinking step
                        next_thought_needed: Whether another thought step is needed
                        thought_number: Current thought number (starts at 1)
                        total_thoughts: Estimated total thoughts needed
                        is_revision: Whether this revises previous thinking
                        revises_thought: Which thought number is being reconsidered
                        branch_from_thought: Branching point thought number
                        branch_id: Branch identifier
                        needs_more_thoughts: If more thoughts are needed

                    Returns:
                        Confirmation message
                    """
                    print(f"[Sequential Thinking {thought_number}/{total_thoughts}] {thought[:100]}...")
                    return f"Thought {thought_number} recorded. Continue: {next_thought_needed}"

                @tool
                def context7_search(library_name: str, topic: str = None) -> str:
                    """Search library documentation using Context7.

                    Args:
                        library_name: Name of the library to search
                        topic: Optional topic to focus on

                    Returns:
                        Library documentation
                    """
                    print(f"[Context7] Searching {library_name} for topic: {topic}")
                    # Context7 ì‹¤ì œ êµ¬í˜„ì€ í–¥í›„ ì¶”ê°€ (í˜„ì¬ëŠ” placeholder)
                    return f"Documentation for {library_name} (topic: {topic})"

                tools = []
                if enable_sequential_thinking:
                    tools.append(sequential_thinking)
                if use_context7:
                    tools.append(context7_search)

                # Tool binding
                llm_with_tools = llm_client.bind_tools(tools)
                print(f"[LLM] Tool calling enabled: sequential_thinking={enable_sequential_thinking}, context7={use_context7}")

                response = llm_with_tools.invoke(prompt)

                # Tool calls ì²˜ë¦¬
                if hasattr(response, 'tool_calls') and response.tool_calls:
                    print(f"[LLM] Tool calls detected: {len(response.tool_calls)}")
                    for tool_call in response.tool_calls:
                        print(f"  - {tool_call.get('name', 'unknown')}: {str(tool_call.get('args', {}))[:100]}")

                # Clean response content to avoid encoding issues
                content = response.content
                return clean_unicode_for_cp949(content) if content else content
            else:
                # Tool ì—†ì´ ì¼ë°˜ í˜¸ì¶œ
                response = llm_client.invoke(prompt)
                # Clean response content to avoid encoding issues
                content = response.content
                return clean_unicode_for_cp949(content) if content else content
        else:
            # Ollama ì‚¬ìš© (tool calling ë¯¸ì§€ì›, ì¼ë°˜ í˜¸ì¶œ)
            model = os.getenv("OLLAMA_MODEL", "gemma2:2b")
            response = ollama.chat(
                model=model,
                messages=[{"role": "user", "content": prompt}]
            )
            print(f"LLM response: {response['message']['content']}")
            return response['message']['content']
    except Exception as e:
        print(f"LLM API í˜¸ì¶œ ì‹¤íŒ¨: {e}")
        import traceback
        traceback.print_exc()
        return f"AI ì‘ë‹µ ìƒì„± ì‹¤íŒ¨: {e}"

def call_ollama(prompt: str, model: str = "gemma3:1b", enable_sequential_thinking: bool = False, use_context7: bool = False) -> str:
    """Ollamaë¥¼ í†µí•œ LLM í˜¸ì¶œ (í•˜ìœ„ í˜¸í™˜ì„±ì„ ìœ„í•´ ìœ ì§€, ë‚´ë¶€ì ìœ¼ë¡œ call_llm ì‚¬ìš©)"""
    return call_llm(prompt, enable_sequential_thinking=enable_sequential_thinking, use_context7=use_context7)

def retrieve_from_rag(query_text: str, num_result_doc: int = 5, retrieval_method: str = "rrf") -> list:
    """RAGë¥¼ í†µí•œ ë¬¸ì„œ ê²€ìƒ‰

    Args:
        query_text: ê²€ìƒ‰ ì¿¼ë¦¬
        num_result_doc: ë°˜í™˜í•  ë¬¸ì„œ ìˆ˜
        retrieval_method: ê²€ìƒ‰ ë°©ë²• ("rrf", "bm25", "knn", "cc")

    Returns:
        ê²€ìƒ‰ ê²°ê³¼ ë¦¬ìŠ¤íŠ¸
    """
    try:
        # í™˜ê²½ ë³€ìˆ˜ì—ì„œ RAG ì„¤ì • ë¡œë“œ
        base_url = os.getenv("RAG_BASE_URL", "http://localhost:8000")
        credential_key = os.getenv("RAG_CREDENTIAL_KEY", "")
        rag_api_key = os.getenv("RAG_API_KEY", "")
        index_name = os.getenv("RAG_INDEX_NAME", "")
        permission_groups = os.getenv("RAG_PERMISSION_GROUPS", "user").split(",")

        # ê²€ìƒ‰ URL ì„¤ì •
        retrieval_urls = {
            "rrf": f"{base_url}/retrieve-rrf",
            "bm25": f"{base_url}/retrieve-bm25",
            "knn": f"{base_url}/retrieve-knn",
            "cc": f"{base_url}/retrieve-cc"
        }

        retrieval_url = retrieval_urls.get(retrieval_method, retrieval_urls["rrf"])

        # í—¤ë” ì„¤ì •
        headers = {
            "Content-Type": "application/json",
            "x-dep-ticket": credential_key,
            "api-key": rag_api_key
        }

        # ìš”ì²­ ë°ì´í„° ì„¤ì •
        fields = {
            "index_name": index_name,
            "permission_groups": permission_groups,
            "query_text": query_text,
            "num_result_doc": num_result_doc,
            "fields_exclude": ["v_merge_title_content"]
        }

        # RAG API í˜¸ì¶œ
        response = requests.post(retrieval_url, headers=headers, json=fields, timeout=30)

        if response.status_code == 200:
            result = response.json()
            print(f"RAG ê²€ìƒ‰ ì™„ë£Œ: {len(result.get('hits', {}).get('hits', []))}ê±´ ê²€ìƒ‰ë¨")
            return result.get('hits', {}).get('hits', [])
        else:
            print(f"RAG API í˜¸ì¶œ ì‹¤íŒ¨: {response.status_code} - {response.text}")
            return []

    except Exception as e:
        print(f"RAG ê²€ìƒ‰ ì‹¤íŒ¨: {e}")
        return []


async def rag_retrieve_bp_cases(domain: str, division: str, proposal_content: str = "") -> dict:
    """RAGë¥¼ í†µí•œ BP ì‚¬ë¡€ ê²€ìƒ‰ (Agent 1ìš© ë˜í¼ í•¨ìˆ˜)

    Args:
        domain: ë¹„ì¦ˆë‹ˆìŠ¤ ë„ë©”ì¸
        division: ë¹„ì¦ˆë‹ˆìŠ¤ êµ¬ì—­
        proposal_content: ì œì•ˆì„œ ë‚´ìš© (ê²€ìƒ‰ ì¿¼ë¦¬ ê°œì„ ìš©)

    Returns:
        dict: {"cases": [list of BP cases]}
    """
    # ì œì•ˆì„œ ë‚´ìš©ì—ì„œ í•µì‹¬ í‚¤ì›Œë“œ ì¶”ì¶œ (ìµœëŒ€ 200ì)
    proposal_snippet = proposal_content[:200] if proposal_content else ""

    # ì œì•ˆì„œ ë‚´ìš©ì„ í¬í•¨í•œ ê²€ìƒ‰ ì¿¼ë¦¬ êµ¬ì„±
    if proposal_snippet:
        query = f"{domain} {division} {proposal_snippet} BP ì‚¬ë¡€"
    else:
        query = f"{domain} {division} BP ì‚¬ë¡€"

    try:
        hits = await asyncio.to_thread(retrieve_from_rag, query, num_result_doc=5)
        cases = []
        for hit in hits:
            source = hit.get("_source", {})
            cases.append({
                "title": source.get("title", "ì œëª© ì—†ìŒ"),
                "tech_type": source.get("tech_type", "AI/ML"),
                "business_domain": source.get("business_domain") or source.get("domain", domain),
                "division": source.get("division", division),
                "problem_as_was": source.get("problem_as_was", source.get("content", "")[:100]),
                "solution_to_be": source.get("solution_to_be", ""),
                "summary": source.get("summary", source.get("content", "")[:200]),
                "tips": source.get("tips", ""),
                "link": source.get("link", "")  # Confluence URL
            })

        # RAG ê²€ìƒ‰ ê²°ê³¼ê°€ ì—†ìœ¼ë©´ ë”ë¯¸ ë°ì´í„° ë°˜í™˜
        if not cases:
            print(f"[DEBUG] RAG ê²€ìƒ‰ ê²°ê³¼ ì—†ìŒ, ë”ë¯¸ ë°ì´í„° ë°˜í™˜")
            cases = get_dummy_bp_cases(domain, division)

        return {"cases": cases}
    except Exception as e:
        print(f"BP ì‚¬ë¡€ ê²€ìƒ‰ ì‹¤íŒ¨: {e}, ë”ë¯¸ ë°ì´í„° ë°˜í™˜")
        return {"cases": get_dummy_bp_cases(domain, division)}


def get_dummy_bp_cases(domain: str, division: str) -> list:
    """RAG ì—°ê²° ì „ í…ŒìŠ¤íŠ¸ìš© ë”ë¯¸ BP ì‚¬ë¡€"""
    return [
        {
            "title": f"{domain} ë¶„ì•¼ AI ê¸°ë°˜ ìë™í™” ì‹œìŠ¤í…œ êµ¬ì¶•",
            "tech_type": "AI/ML - ìì—°ì–´ì²˜ë¦¬",
            "business_domain": domain,
            "division": division,
            "problem_as_was": f"{domain} ì—…ë¬´ì—ì„œ ìˆ˜ì‘ì—… ì²˜ë¦¬ë¡œ ì¸í•œ ì‹œê°„ ì†Œìš” ë° ì˜¤ë¥˜ ë°œìƒ (í•˜ë£¨ í‰ê·  4ì‹œê°„ ì†Œìš”)",
            "solution_to_be": "AI ê¸°ë°˜ ìë™ ë¶„ë¥˜ ë° ì²˜ë¦¬ ì‹œìŠ¤í…œ ë„ì…ìœ¼ë¡œ ì²˜ë¦¬ ì‹œê°„ 80% ë‹¨ì¶• ë° ì •í™•ë„ 95% ë‹¬ì„±",
            "summary": f"{domain} ë¶„ì•¼ì— AI ìë™í™”ë¥¼ ë„ì…í•˜ì—¬ ì—…ë¬´ íš¨ìœ¨ì„±ì„ í¬ê²Œ í–¥ìƒì‹œí‚¨ ì‚¬ë¡€. 6ê°œì›” ë‚´ ROI 200% ë‹¬ì„±",
            "tips": "ì´ˆê¸° ë°ì´í„° í’ˆì§ˆ í™•ë³´ê°€ ì¤‘ìš”. íŒŒì¼ëŸ¿ í”„ë¡œì íŠ¸ë¡œ ì‹œì‘í•˜ì—¬ ì ì§„ì  í™•ëŒ€ ê¶Œì¥",
            "link": ""  # ë”ë¯¸ ë°ì´í„°ëŠ” ë§í¬ ì—†ìŒ
        },
        {
            "title": f"{division} {domain} ë°ì´í„° ë¶„ì„ í”Œë«í¼ êµ¬ì¶•",
            "tech_type": "AI/ML - ì˜ˆì¸¡ ë¶„ì„",
            "business_domain": domain,
            "division": division,
            "problem_as_was": "ë¶„ì‚°ëœ ë°ì´í„°ë¡œ ì¸í•œ ì˜ì‚¬ê²°ì • ì§€ì—° ë° ì¸ì‚¬ì´íŠ¸ ë¶€ì¡±",
            "solution_to_be": "í†µí•© ë°ì´í„° ë¶„ì„ í”Œë«í¼ êµ¬ì¶•ìœ¼ë¡œ ì‹¤ì‹œê°„ ì¸ì‚¬ì´íŠ¸ ì œê³µ ë° ì˜ˆì¸¡ ì •í™•ë„ í–¥ìƒ",
            "summary": f"{division} ì‚¬ì—…ë¶€ì˜ {domain} ë°ì´í„°ë¥¼ í†µí•© ë¶„ì„í•˜ì—¬ ì˜ì‚¬ê²°ì • ì†ë„ 3ë°° í–¥ìƒ",
            "tips": "ë°ì´í„° ê±°ë²„ë„ŒìŠ¤ ì²´ê³„ë¥¼ ë¨¼ì € ìˆ˜ë¦½í•œ í›„ í”Œë«í¼ êµ¬ì¶• ì‹œì‘",
            "link": ""
        },
        {
            "title": f"{domain} ìµœì í™”ë¥¼ ìœ„í•œ ë¨¸ì‹ ëŸ¬ë‹ ëª¨ë¸ ì ìš©",
            "tech_type": "AI/ML - ìµœì í™”",
            "business_domain": domain,
            "division": division,
            "problem_as_was": "ê²½í—˜ ê¸°ë°˜ ì˜ì‚¬ê²°ì •ìœ¼ë¡œ ì¸í•œ ìµœì í™” í•œê³„ ë° ë¦¬ì†ŒìŠ¤ ë‚­ë¹„",
            "solution_to_be": "ML ê¸°ë°˜ ìµœì í™” ëª¨ë¸ë¡œ ë¦¬ì†ŒìŠ¤ í™œìš©ë¥  30% ê°œì„  ë° ë¹„ìš© ì ˆê°",
            "summary": f"{domain} ì—…ë¬´ ìµœì í™”ë¥¼ ìœ„í•œ ML ëª¨ë¸ ê°œë°œ ë° ì ìš© ì„±ê³µ ì‚¬ë¡€",
            "tips": "ë„ë©”ì¸ ì „ë¬¸ê°€ì™€ ë°ì´í„° ì‚¬ì´ì–¸í‹°ìŠ¤íŠ¸ì˜ ê¸´ë°€í•œ í˜‘ì—…ì´ ì„±ê³µì˜ í•µì‹¬",
            "link": ""
        }
    ]


async def process_confluence_pages_sequentially(job_ids: list, page_list: list):
    """Confluence í˜ì´ì§€ë“¤ì„ ìˆœì°¨ì ìœ¼ë¡œ ì²˜ë¦¬"""
    print(f"=== Sequential processing started for {len(job_ids)} pages ===")

    all_reports = []
    main_job_id = job_ids[0]  # ì²« ë²ˆì§¸ job_idë¥¼ ë©”ì¸ WebSocketìœ¼ë¡œ ì‚¬ìš©
    main_ws_key = str(main_job_id)  # WebSocket dictëŠ” ë¬¸ìì—´ í‚¤ë¥¼ ì‚¬ìš©

    for idx, job_id in enumerate(job_ids):
        page_info = page_list[idx]
        print(f"\n{'='*80}")
        print(f"Processing page {idx+1}/{len(job_ids)}: {page_info['title']} (Job ID: {job_id})")
        print(f"{'='*80}\n")

        # UIì— í˜„ì¬ ì²˜ë¦¬ ì¤‘ì¸ í˜ì´ì§€ ì•Œë¦¼
        if main_ws_key in active_connections:
            await active_connections[main_ws_key].send_json({
                "type": "page_progress",
                "current_page": idx + 1,
                "total_pages": len(job_ids),
                "page_title": page_info['title'],
                "page_id": page_info['id'],
                "job_id": job_id,
                "status": "processing",
                "message": f"ğŸ“„ í˜ì´ì§€ {idx+1}/{len(job_ids)} ë¶„ì„ ì¤‘: {page_info['title']}",
                "reset_agents": idx > 0
            })

        # ê° í˜ì´ì§€ ì²˜ë¦¬ (6ê°œ ì—ì´ì „íŠ¸ ì „ì²´ í”Œë¡œìš°)
        await process_review(
            job_id,
            ws_job_key=main_ws_key,
            send_final_report=False
        )

        # ì²˜ë¦¬ ì™„ë£Œëœ jobì˜ ìµœì¢… report ê°€ì ¸ì˜¤ê¸°
        job_data = get_job(job_id)
        if job_data and job_data.get('report'):
            page_report_data = {
                "page_title": page_info['title'],
                "page_id": page_info['id'],
                "job_id": job_id,
                "report": job_data['report'],
                "decision": job_data.get('llm_decision'),
                "decision_reason": (job_data.get('metadata') or {}).get('final_decision', {}).get('reason')
            }
            all_reports.append(page_report_data)

            # UIì— í˜ì´ì§€ë³„ ì™„ë£Œ ê²°ê³¼ ì¦‰ì‹œ ì „ì†¡
            if main_ws_key in active_connections:
                await active_connections[main_ws_key].send_json({
                    "status": "page_completed",
                    "current_page": idx + 1,
                    "total_pages": len(job_ids),
                    "page_title": page_info['title'],
                    "page_id": page_info['id'],
                    "page_report": job_data['report'],
                    "page_decision": job_data.get('llm_decision'),
                    "page_decision_reason": (job_data.get('metadata') or {}).get('final_decision', {}).get('reason')
                })

        # UIì— í˜ì´ì§€ ì™„ë£Œ ì•Œë¦¼
        if main_ws_key in active_connections:
            await active_connections[main_ws_key].send_json({
                "type": "page_progress",
                "current_page": idx + 1,
                "total_pages": len(job_ids),
                "page_title": page_info['title'],
                "page_id": page_info['id'],
                "job_id": job_id,
                "status": "completed",
                "message": f"âœ… í˜ì´ì§€ {idx+1}/{len(job_ids)} ì™„ë£Œ: {page_info['title']}"
            })

        print(f"\n[OK] Completed page {idx+1}/{len(job_ids)}: {page_info['title']}\n")

    # ëª¨ë“  í˜ì´ì§€ ì²˜ë¦¬ ì™„ë£Œ í›„ í†µí•© ë¦¬í¬íŠ¸ ìƒì„±
    print(f"\n{'='*80}")
    print(f"All {len(job_ids)} pages processed successfully")
    print(f"{'='*80}\n")

    # ì²« ë²ˆì§¸ jobì˜ WebSocketìœ¼ë¡œ ìµœì¢… ì™„ë£Œ ì•Œë¦¼
    if job_ids and str(job_ids[0]) in active_connections:
        combined_report = "# ğŸ“š Confluence í˜ì´ì§€ë³„ ê²€í†  ê²°ê³¼\n\n"
        combined_report += f"**ì´ {len(all_reports)}ê°œ í˜ì´ì§€ ë¶„ì„ ì™„ë£Œ**\n\n"
        combined_report += "---\n\n"

        for idx, report_data in enumerate(all_reports, 1):
            combined_report += f"## {idx}. {report_data['page_title']}\n\n"
            combined_report += report_data['report']
            decision_line = report_data.get('decision')
            reason = report_data.get('decision_reason')
            if decision_line:
                combined_report += f"\n\n**ê²°ì •:** {decision_line}"
                if reason:
                    combined_report += f"<br>ì´ìœ : {reason}"
            combined_report += "\n\n---\n\n"

        decisions_summary = [
            {
                "page_title": item.get("page_title"),
                "decision": item.get("decision"),
                "reason": item.get("decision_reason"),
            }
            for item in all_reports
        ]

        await active_connections[str(job_ids[0])].send_json({
            "status": "completed",
            "report": combined_report,
            "page_count": len(all_reports),
            "decisions": decisions_summary,
        })

async def process_review(job_id: int, ws_job_key: str | None = None, send_final_report: bool = True):
    """ë°±ê·¸ë¼ìš´ë“œ ê²€í†  í”„ë¡œì„¸ìŠ¤ - 6ê°œ ì—ì´ì „íŠ¸ ì „ì²´ í”Œë¡œìš°"""
    print(f"=== process_review ENTRY for job {job_id} ===")
    ws = None
    ws_key = ws_job_key or str(job_id)
    try:
        print(f"process_review started for job {job_id}")
        job = get_job(job_id)
        print(f"Job data retrieved for job {job_id}")
        if not job:
            print(f"Job {job_id} not found")
            return

        # HITL ë‹¨ê³„ ì„¤ì • ê°€ì ¸ì˜¤ê¸°
        hitl_stages = job.get("hitl_stages", [])
        print(f"HITL stages enabled: {hitl_stages}")

        # HITL ì¬ì‹œë„ ì¹´ìš´í„° ì´ˆê¸°í™” (ê° ì—ì´ì „íŠ¸ë‹¹ ìµœëŒ€ 3íšŒ)
        hitl_retry_counts = {2: 0, 3: 0, 4: 0, 5: 0, 6: 0}
        MAX_HITL_RETRIES = 3

        # ì‚¬ìš©ì í”¼ë“œë°± ìˆ˜ì§‘ìš© ë”•ì…”ë„ˆë¦¬ (Agent 7ì— ì „ë‹¬)
        user_feedbacks = {}

        # Wait for WebSocket connection (up to 3 seconds)
        for i in range(30):
            ws = active_connections.get(ws_key)
            if ws:
                print(f"WebSocket connected on attempt {i+1}")
                break
            await asyncio.sleep(0.1)

        print(f"WebSocket connection: {ws}")
        print(f"Active connections: {list(active_connections.keys())}")
        domain = job.get("domain", "")
        division = job.get("division", "")
        print(f"Domain: {domain}, Division: {division}")

        # Agent 1: BP Case Scouter
        bp_cases = await run_bp_scouter(job_id, job, ws, domain, division,
                                         rag_retrieve_bp_cases, get_job, update_job_status)

        # Agent 2: Objective Reviewer
        objective_review = await run_objective_reviewer(job_id, job, ws, hitl_stages, hitl_retry_counts, bp_cases,
                                                          call_ollama, get_job, update_job_status, reset_feedback_state)
        # Agent 2 í”¼ë“œë°± ìˆ˜ì§‘
        if 2 in hitl_stages:
            job_data = get_job(job_id)
            if job_data and job_data.get("metadata", {}).get("user_feedbacks", {}).get(2):
                user_feedbacks[2] = job_data["metadata"]["user_feedbacks"][2]

        # Agent 3: Data Analyzer
        data_analysis = await run_data_analyzer(job_id, job, ws, hitl_stages, hitl_retry_counts,
                                                  call_ollama, get_job, update_job_status, reset_feedback_state)
        # Agent 3 í”¼ë“œë°± ìˆ˜ì§‘
        if 3 in hitl_stages:
            job_data = get_job(job_id)
            if job_data and job_data.get("metadata", {}).get("user_feedbacks", {}).get(3):
                user_feedbacks[3] = job_data["metadata"]["user_feedbacks"][3]

        # Agent 4: Risk Analyzer
        risk_analysis = await run_risk_analyzer(job_id, job, ws, hitl_stages, hitl_retry_counts,
                                                  call_ollama, get_job, update_job_status, reset_feedback_state)
        # Agent 4 í”¼ë“œë°± ìˆ˜ì§‘
        if 4 in hitl_stages:
            job_data = get_job(job_id)
            if job_data and job_data.get("metadata", {}).get("user_feedbacks", {}).get(4):
                user_feedbacks[4] = job_data["metadata"]["user_feedbacks"][4]

        # Agent 5: ROI Estimator
        roi_estimation = await run_roi_estimator(job_id, job, ws, hitl_stages, hitl_retry_counts,
                                                   call_ollama, get_job, update_job_status, reset_feedback_state)
        # Agent 5 í”¼ë“œë°± ìˆ˜ì§‘
        if 5 in hitl_stages:
            job_data = get_job(job_id)
            if job_data and job_data.get("metadata", {}).get("user_feedbacks", {}).get(5):
                user_feedbacks[5] = job_data["metadata"]["user_feedbacks"][5]

        # Agent 6: Final Generator
        # Don't send final report yet - wait for Agent 7
        # Agent 2~5 í”¼ë“œë°±ì„ Agent 6ì— ì „ë‹¬
        collected_feedbacks = {k: v for k, v in user_feedbacks.items() if k in [2, 3, 4, 5]}
        print(f"[DEBUG] Feedbacks from Agent 2~5 to pass to Agent 6: {collected_feedbacks}")

        await run_final_generator(
            job_id, job, ws, hitl_stages, hitl_retry_counts,
            objective_review, data_analysis, risk_analysis, roi_estimation, bp_cases,
            call_ollama, call_llm, get_job, update_job_status, reset_feedback_state,
            send_final_report=False,  # Agent 7 will send the final report
            ws_key=ws_key,
            active_connections=active_connections,
            user_feedbacks=collected_feedbacks
        )

        # Agent 7: Proposal Improver - Generate improved proposal
        # Get final_recommendation from Agent 6
        latest_job = get_job(job_id)
        final_recommendation = latest_job.get("metadata", {}).get("agent_results", {}).get("final_recommendation", "")

        # Agent 6 í”¼ë“œë°± ìˆ˜ì§‘
        if 6 in hitl_stages:
            job_data = get_job(job_id)
            if job_data and job_data.get("metadata", {}).get("user_feedbacks", {}).get(6):
                user_feedbacks[6] = job_data["metadata"]["user_feedbacks"][6]

        print(f"[DEBUG] User feedbacks collected: {user_feedbacks}")

        improved_proposal = await run_proposal_improver(
            job_id, job, ws,
            objective_review, data_analysis, risk_analysis, roi_estimation, final_recommendation, bp_cases,
            call_ollama, get_job, update_job_status, user_feedbacks
        )

        # Update final report with improved proposal section
        # Always send final report after Agent 7
        if improved_proposal:
            latest_job_data = get_job(job_id)
            metadata = latest_job_data.get("metadata", {}).copy()
            current_report = metadata.get("report", "")

            # Add improved proposal section to report
            # Clean the improved proposal text to avoid encoding issues
            cleaned_proposal = improved_proposal.replace('\u202f', ' ').replace('\u00a0', ' ')
            improved_section = f"""
        <div class="accordion-item">
            <div class="accordion-header" onclick="toggleAccordion('section7')">
                <span>7. ê°œì„ ëœ ì§€ì›ì„œ ì œì•ˆ</span>
                <span class="accordion-icon">â–¼</span>
            </div>
            <div id="section7" class="accordion-content" style="display: block;">
                <div style="background: #f8f9fa; padding: 15px; border-left: 4px solid #28a745;">
                    {cleaned_proposal.replace(chr(10), '<br>')}
                </div>
                <div style="margin-top: 15px; text-align: right;">
                    <button onclick="window.location.href='/api/export/improved-proposal/{job_id}'"
                            style="background-color: #28a745; color: white; border: none; padding: 10px 20px; border-radius: 5px; cursor: pointer; font-size: 14px;">
                        ğŸ“„ ê°œì„ ëœ ì§€ì›ì„œ PDF ë‹¤ìš´ë¡œë“œ
                    </button>
                </div>
            </div>
        </div>
    """

            # Insert improved section before the final closing </div> tag
            # Find the last </div> and insert before it
            last_div_pos = current_report.rfind("</div>")
            if last_div_pos != -1:
                updated_report = current_report[:last_div_pos] + improved_section + current_report[last_div_pos:]
            else:
                updated_report = current_report
            metadata["report"] = updated_report

            update_job_status(job_id, "completed", metadata=metadata)

            # Send updated report via WebSocket (only if send_final_report is True)
            if send_final_report:
                target_ws = ws or (active_connections.get(ws_key) if active_connections and ws_key else None)
                if target_ws:
                    human_decision_value = latest_job_data.get("decision") or latest_job_data.get("human_decision")
                    decision_value = metadata.get("final_decision", {}).get("decision", "ë³´ë¥˜")
                    decision_reason = metadata.get("final_decision", {}).get("reason", "")

                    await target_ws.send_json({
                        "status": "completed",
                        "agent": "Proposal_Improver",
                        "message": "ê°œì„ ëœ ì§€ì›ì„œ ìƒì„± ì™„ë£Œ",
                        "report": updated_report,
                        "decision": decision_value,
                        "decision_reason": decision_reason,
                        "human_decision": human_decision_value,
                        "current_page": 1,
                        "total_pages": 1,
                    })
                    print(f"[INFO] Sent final report for job {job_id}")
            else:
                print(f"[INFO] Skipped sending final report for job {job_id} (multi-page mode)")

    except Exception as e:
        print(f"!!! ERROR in review process: {e}")
        import traceback
        traceback.print_exc()
        if ws:
            try:
                await ws.send_json({"status": "error", "message": f"Error: {str(e)}"})
            except:
                pass
        update_job_status(job_id, "error")
    finally:
        print(f"=== process_review EXIT for job {job_id} ===")

@app.websocket("/ws/{job_id}")
async def websocket_endpoint(websocket: WebSocket, job_id: str):
    """WebSocket ì‹¤ì‹œê°„ ì§„í–‰ìƒí™© ì—…ë°ì´íŠ¸"""
    await websocket.accept()
    active_connections[job_id] = websocket

    try:
        while True:
            # í´ë¼ì´ì–¸íŠ¸ë¡œë¶€í„° ë©”ì‹œì§€ ìˆ˜ì‹  (keep-alive)
            data = await websocket.receive_text()
            # ì—ì½”ë°± (ì—°ê²° ìœ ì§€)
            await websocket.send_json({"type": "pong"})
    except WebSocketDisconnect:
        print(f"WebSocket ì—°ê²° ì¢…ë£Œ: {job_id}")
        del active_connections[job_id]
    except Exception as e:
        print(f"WebSocket ì—ëŸ¬: {e}")
        if job_id in active_connections:
            del active_connections[job_id]


if __name__ == "__main__":
    print(f"Server starting at http://{HOST}:{PORT}")
    uvicorn.run(app, host=HOST, port=PORT)
