## 1. 개요

본 문서는 **AI 과제 지원서를 자동으로 검토하고 개선 제안을 제공하는 다중 AI 에이전트 시스템**의 기술적·기능적 요구사항을 명세화한다.  
시스템은 도메인 지식이 없는 AI 컨설턴트가 고품질의 검토를 수행할 수 있도록 설계되며, 인간(Human)의 피드백을 반영하는 **Human-in-the-Loop(HITL)** 프로세스를 핵심으로 한다.  
모든 에이전트는 **거대 언어 모델(Large Language Model, LLM)** 기반으로 구현되며, **Best Practice(BP) 사례 데이터베이스**를 공통 지식 기반으로 활용한다.

---

## 2. 목표

- AI 과제 지원서의 각 항목이 **문제 해결에 충분한지** 자동 검토
- 도메인 전문가가 아닌 AI 컨설턴트도 **신뢰성 높은 검토** 수행 가능하도록 설계
- **BP 사례 기반**으로 현실성·실현 가능성 평가 및 개선 제안
- **HITL 프로세스**를 통해 인간의 피드백을 반영하여 최종 지원서 완성
- **HITL 반복 횟수 제한**하여도 검토 완료 가능하도록 성능 최적화

---

## 3. 시스템 아키텍처

### 3.1 전체 구조
- **오케스트레이션 기반 다중 에이전트 시스템**
- 중앙 **검토 오케스트레이터(Review Orchestrator)** 가 워크플로우 제어
- 모든 에이전트는 **LLM 기반**으로 동작
- **RAG(Retrieval-Augmented Generation)** 아키텍처 기반 BP 사례 활용
- **병렬 실행** 가능 (속도 최적화)

### 3.2 주요 구성 요소

| 구성 요소 | 설명 |
|----------|------|
| **벡터 DB** | BP 사례 임베딩 저장 (RAGaaS 활용, 검색 코드는 appendix/rag_retrieve.py 참조) |
| **검토 오케스트레이터** | 에이전트 호출, HITL 조율, 최종 문서 생성 관리 |
| **AI 에이전트 집합** | 총 6개 전문 에이전트 (아래 4.1 참조) |
| **HITL 인터페이스** | 사용자에게 예시 제시 → 피드백 수신 → 지원서 수정 반영 |
| **PDF, md 생성 모듈** | 최종 지원서를 PDF, md로 변환 |

---

## 4. 에이전트 정의

### 4.1 에이전트 목록 및 역할

| 에이전트 이름 | 역할 | 입력 | 출력 |
|--------------|------|------|------|
| **BP 사례 탐색 에이전트** | 지원서 내용 기반 관련 BP 사례 3~5개 검색 및 요약 | 지원서 전문 | 구조화된 BP 사례 목록 |
| **목표 & 기대 효과 검토 에이전트** | 지원서의 핵심인 “왜 AI를 도입하는가?”와 “어떤 효과를 기대하는가?”를 평가.정량화,현실성,명확성 검토는 반드시 필요. | 지원서 + BP 사례 | 목표/기대 효과 수정 제안 |
| **데이터 & I/O 형식 검토 에이전트** | AI 프로젝트 성패는 데이터에 달려 있음. 데이터 유형, 양, 라벨링 상태, 입력/출력 형식은 기술적 실현 가능성을 판단하는 핵심 요소 | 지원서 + BP 사례 | 데이터 준비 상태 평가 및 개선 제안 |
| **리스크 & 실패 사례 분석 에이전트** | 유사 과제에서 발생한 실패 사례, 공통 리스크(데이터 부족, 라벨링 오류, 인프라 미비, 사용자 저항 등)를 BP DB에서 추출 | 지원서 + BP 사례 | 지원서 내용을 바탕으로 잠재적 리스크 예측 및 완화 전략 제안 |
| **ROI & 실행 로드맵 추정 에이전트** | 기대 효과(예: 불량률 20% 감소, 비용 절감)를 바탕으로 초기 투자 대비 수익(ROI) 산정 | 지원서 + BP 사례 | BP 사례의 구현 기간/자원을 참고해 3단계 실행 로드맵(Proof of Concept → Pilot → Full Scale) 제안 |
| **최종 검토 & PDF, MD 생성 에이전트** | 모든 에이전트 결과 종합, HITL 요청, 최종 지원서 PDF, MD 생성 | 모든 에이전트 출력 | PDF 파일 + HITL 승인 요청 |

> ✅ **공통 요구사항**: 모든 에이전트는 BP 사례를 Context로 활용하며, RAG를 통해 사실 기반(fact-based) 응답 생성.

---

## 5. Human-in-the-Loop (HITL) 프로세스

### 5.1 프로세스 흐름
1. 각 전문 에이전트가 **구체적인 예시와 함께 수정 제안** 생성  
   > 예: “기대 효과 ‘비용 절감’은 모호합니다. BP 사례 #3에서는 ‘연간 1.2억 원 절감’으로 정량화했습니다. 이와 유사하게 수정하시겠습니까?”
2. 사용자(도메인 전문가)는 제시된 예시에서 **‘맞는 부분’과 ‘틀린 부분’을 명시적으로 피드백**  
   > 예: “비용 절감은 맞지만, 금액은 8천만 원이 적절합니다.”
3. 시스템은 피드백을 반영하여 **해당 항목 재생성**
4. 최종 단계에서 **PDF 생성 전 최종 승인 요청**

### 5.2 HITL 구현 방식
- **LangGraph**의 `interrupt_before` 기능 활용
- 피드백은 **구조화된 형식**(예: JSON)으로 수집 권장
- HITL은 **최종 단계 및 필요 시 선택적 단계**에서만 발생 (성능 저하 최소화)

---

## 6. 데이터 및 지식 관리

### 6.1 BP 사례 데이터베이스
- **형식**: JSON (필드: `title`, `background`, `data_status`, `results`, `success_factors`, `url`)
- **출처**: 사내 성공 사례, 공개된 산업 BP, 학술/컨설팅 리포트
- **업데이트 정책**: 정기적 수동 업데이트 + 외부 검색 연동 (RAG 미매칭 시)
---

## 7. 성능 및 제약 조건

| 항목 | 요구사항 |
|------|--------|
| **응답 시간** | 평균 3~5분 이내 (HITL 반복 제외) |
| **LLM 규모** | 거대 모델 사용 (대규모 데이터센터 기반) |
| **병렬 처리** | 가능 (오케스트레이터가 병렬 호출 지원) |
| **HITL 반복 제한** | 최대 3회 (무한 루프 방지) |
| **확장성** | 에이전트 추가/삭제 용이 (역할 기반 설계) |

---

## 8. 출력 산출물

- **최종 AI 과제 지원서**(Markdown 또는 HTML)
- **검토 요약 보고서**(각 항목별 개선 내역 및 근거)
- **PDF 파일**: 최종 승인 후 자동 생성

---

## 9. 기술 스택 (제안)

| 계층 | 기술 |
|------|------|
| **오케스트레이션** | LangGraph 또는 AutoGen |
| **LLM** | gpt-oss:120b |
| **RAG** | RAGaaS, appendix/rag_retrieve.py 참조 |
| **평가** | RAGAS, Langfuse |
| **PDF 생성** | WeasyPrint 또는 Puppeteer |

---

## 10. 성공 지표 (KPIs)

| 카테고리 | 지표 |
|--------|------|
| **품질** | HITL 승인률 ≥ 90%, 사용자 만족도 ≥ 4.5/5 |
| **성능** | 평균 처리 시간 ≤ 300초, 동시 요청 처리 가능 |
| **기술** | RAG Faithfulness ≥ 0.85, 생성 오류율 < 5% |
| **운영** | BP 사례 활용률 ≥ 95%, 에이전트 실패율 < 2% |

---

## 11. 차후 고려사항

- **강화 학습(RLHF)**: HITL 피드백을 통한 에이전트 자동 개선
- **자가 진단 에이전트**: 성능 저하 감지 및 자동 재학습 트리거
- **멀티 도메인 확장**: 제조, 금융, 물류 등 산업별 BP 사례 분리 관리
- **API 연동**: 외부 시스템(예: Jira, Confluence)과의 통합

---

## 12. 기타 사항
- **보안**: 추후 별도 적용
- **fallback**: 추후 별도 적용
- **참고**: 코드 디버깅이 쉽도록 보안 이나 fallback같이 본래의 코드의 실행 결과에 영향을 줄수 있는 부분은 구현하지 않는다.