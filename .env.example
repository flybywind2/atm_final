# LLM 설정
# LLM_PROVIDER: "ollama" 또는 "internal"
LLM_PROVIDER=ollama

# Ollama 설정 (LLM_PROVIDER=ollama인 경우)
OLLAMA_BASE_URL=http://localhost:11434
OLLAMA_MODEL=gemma2:1b

# Internal LLM 설정 (LLM_PROVIDER=internal인 경우)
INTERNAL_BASE_URL=https://model1.openai.com/v1
INTERNAL_MODEL=llama4 maverick
INTERNAL_CREDENTIAL_KEY=your_credential_key
INTERNAL_SYSTEM_NAME=System_Name
INTERNAL_USER_ID=ID

# RAG 설정
RAG_BASE_URL=http://localhost:8000
RAG_CREDENTIAL_KEY=your_credential_key
RAG_API_KEY=your_rag_api_key
RAG_INDEX_NAME=your_index_name
RAG_PERMISSION_GROUPS=user

# Confluence Data Center 설정
CONFLUENCE_BASE_URL=https://localhost:8090
CONFLUENCE_USERNAME=your_username
CONFLUENCE_API_TOKEN=your_api_token

# VLM (Vision Language Model) 설정 - Internal model3 (gemma3:27b, vLLM)
# VLM을 사용하여 이미지 처리 (PDF, DOCX, Confluence 이미지 등)
VLM_ENABLED=true
VLM_BASE_URL=https://model3.openai.com/v1
VLM_MODEL=gemma3:27b
VLM_API_KEY=your_openai_api_key
VLM_CREDENTIAL_KEY=your_credential_key
VLM_SYSTEM_NAME=System_Name
VLM_USER_ID=ID