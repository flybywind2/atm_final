"""LLM initialization and calling functions"""
import os
import uuid
import ollama


# LLM ì„¤ì • ë° ì´ˆê¸°í™”
LLM_PROVIDER = os.getenv("LLM_PROVIDER", "ollama")
llm_client = None


def init_llm():
    """í™˜ê²½ ë³€ìˆ˜ì— ë”°ë¼ LLM í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™”"""
    global llm_client

    if LLM_PROVIDER == "internal":
        # Internal LLM ì„¤ì • (lazy import to avoid pydantic version issues)
        from langchain_openai import ChatOpenAI

        base_url = os.getenv("INTERNAL_BASE_URL")
        model = os.getenv("INTERNAL_MODEL")
        credential_key = os.getenv("INTERNAL_CREDENTIAL_KEY")
        system_name = os.getenv("INTERNAL_SYSTEM_NAME")
        user_id = os.getenv("INTERNAL_USER_ID")

        llm_client = ChatOpenAI(
            base_url=base_url,
            model=model,
            default_headers={
                "x-dep-ticket": credential_key,
                "Send-System-Name": system_name,
                "User-ID": user_id,
                "User-Type": "AD",
                "Prompt-Msg-Id": str(uuid.uuid4()),
                "Completion-Msg-Id": str(uuid.uuid4()),
            },
        )
        print(f"Internal LLM initialized: {model}")
    else:
        # Ollama ì„¤ì •
        llm_client = "ollama"  # OllamaëŠ” ì§ì ‘ í•¨ìˆ˜ë¡œ í˜¸ì¶œ
        print(f"Ollama LLM initialized: {os.getenv('OLLAMA_MODEL', 'gemma2:2b')}")


def clean_unicode_for_cp949(text: str) -> str:
    """CP949 ì¸ì½”ë”©ì—ì„œ ë¬¸ì œê°€ ë˜ëŠ” ìœ ë‹ˆì½”ë“œ ë¬¸ìë¥¼ ì•ˆì „í•˜ê²Œ ì œê±°"""
    if not text:
        return text

    # CP949ë¡œ ì¸ì½”ë”© ê°€ëŠ¥í•œ ë¬¸ìë§Œ ìœ ì§€
    try:
        # ë¨¼ì € CP949ë¡œ ì¸ì½”ë”© ì‹œë„
        text.encode('cp949')
        return text
    except UnicodeEncodeError:
        # ì¸ì½”ë”© ì‹¤íŒ¨ ì‹œ ë¬¸ìë³„ë¡œ ì²˜ë¦¬
        cleaned = []
        for char in text:
            try:
                char.encode('cp949')
                cleaned.append(char)
            except UnicodeEncodeError:
                # CP949ë¡œ ì¸ì½”ë”©í•  ìˆ˜ ì—†ëŠ” ë¬¸ìëŠ” ê³µë°± ë˜ëŠ” ? ë¡œ ëŒ€ì²´
                if char.isspace():
                    cleaned.append(' ')
                else:
                    cleaned.append('?')
        return ''.join(cleaned)


def call_llm(prompt: str, enable_sequential_thinking: bool = False, use_context7: bool = False) -> str:
    """í†µí•© LLM í˜¸ì¶œ í•¨ìˆ˜

    Args:
        prompt: LLMì— ì „ë‹¬í•  í”„ë¡¬í”„íŠ¸
        enable_sequential_thinking: Sequential Thinking MCP í™œì„±í™” ì—¬ë¶€
        use_context7: Context7 tool í™œì„±í™” ì—¬ë¶€

    Returns:
        LLM ì‘ë‹µ ë¬¸ìì—´
    """
    try:
        if LLM_PROVIDER == "internal":
            # Internal LLM ì‚¬ìš© (tool calling ì§€ì›)
            if enable_sequential_thinking or use_context7:
                # Tool calling í™œì„±í™”
                import json
                from langchain_core.messages import HumanMessage, AIMessage, ToolMessage

                # Sequential thinking tool definition
                sequential_thinking_tool = {
                    "type": "function",
                    "function": {
                        "name": "sequentialthinking",
                        "description": """A detailed tool for dynamic and reflective problem-solving through thoughts.
This tool helps analyze problems through a flexible thinking process that can adapt and evolve.
Each thought can build on, question, or revise previous insights as understanding deepens.""",
                        "parameters": {
                            "type": "object",
                            "properties": {
                                "thought": {
                                    "type": "string",
                                    "description": "Your current thinking step"
                                },
                                "nextThoughtNeeded": {
                                    "type": "boolean",
                                    "description": "Whether another thought step is needed"
                                },
                                "thoughtNumber": {
                                    "type": "integer",
                                    "description": "Current thought number",
                                    "minimum": 1
                                },
                                "totalThoughts": {
                                    "type": "integer",
                                    "description": "Estimated total thoughts needed",
                                    "minimum": 1
                                },
                                "isRevision": {
                                    "type": "boolean",
                                    "description": "Whether this revises previous thinking"
                                },
                                "revisesThought": {
                                    "type": "integer",
                                    "description": "Which thought is being reconsidered"
                                },
                                "branchFromThought": {
                                    "type": "integer",
                                    "description": "Branching point thought number"
                                },
                                "branchId": {
                                    "type": "string",
                                    "description": "Branch identifier"
                                },
                                "needsMoreThoughts": {
                                    "type": "boolean",
                                    "description": "If more thoughts are needed"
                                }
                            },
                            "required": ["thought", "nextThoughtNeeded", "thoughtNumber", "totalThoughts"]
                        }
                    }
                }

                tools = []
                if enable_sequential_thinking:
                    tools.append(sequential_thinking_tool)

                # Tool binding
                llm_with_tools = llm_client.bind_tools(tools)
                print(f"[LLM] Tool calling enabled: sequential_thinking={enable_sequential_thinking}, context7={use_context7}")

                # Handle sequential thinking loop
                messages = [HumanMessage(content=prompt)]
                thoughts = []

                while True:
                    response = llm_with_tools.invoke(messages)

                    # Tool calls ì²˜ë¦¬
                    if hasattr(response, 'tool_calls') and response.tool_calls:
                        print(f"[LLM] Tool calls detected: {len(response.tool_calls)}")

                        for tool_call in response.tool_calls:
                            if tool_call['name'] == 'sequentialthinking':
                                args = tool_call['args']
                                thoughts.append(args)

                                print(f"ğŸ’­ Thought {args['thoughtNumber']}/{args['totalThoughts']}: {args['thought'][:100]}...")

                                # Add tool response to messages
                                messages.append(AIMessage(content="", tool_calls=[tool_call]))
                                messages.append(ToolMessage(
                                    tool_call_id=tool_call['id'],
                                    content=json.dumps({"status": "thought_recorded"})
                                ))

                                # Check if more thoughts are needed
                                if not args.get('nextThoughtNeeded', False):
                                    print(f"âœ… Thinking complete! Total thoughts: {len(thoughts)}")
                                    # Get final answer
                                    final_response = llm_client.invoke(messages + [HumanMessage(content="Please provide your final answer based on the thoughts above.")])
                                    return clean_unicode_for_cp949(final_response.content) if final_response.content else ""
                    else:
                        # No tool call, return response
                        content = response.content
                        return clean_unicode_for_cp949(content) if content else content
            else:
                # Tool ì—†ì´ ì¼ë°˜ í˜¸ì¶œ
                response = llm_client.invoke(prompt)
                # Clean response content to avoid encoding issues
                content = response.content
                return clean_unicode_for_cp949(content) if content else content
        else:
            # Ollama ì‚¬ìš© (tool calling ë¯¸ì§€ì›, ì¼ë°˜ í˜¸ì¶œ)
            model = os.getenv("OLLAMA_MODEL", "gemma2:2b")
            response = ollama.chat(
                model=model,
                messages=[{"role": "user", "content": prompt}]
            )
            print(f"LLM response: {response['message']['content']}")
            return response['message']['content']
    except Exception as e:
        print(f"LLM API í˜¸ì¶œ ì‹¤íŒ¨: {e}")
        import traceback
        traceback.print_exc()
        return f"AI ì‘ë‹µ ìƒì„± ì‹¤íŒ¨: {e}"


def call_ollama(prompt: str, model: str = "gemma3:1b", enable_sequential_thinking: bool = False, use_context7: bool = False) -> str:
    """Ollamaë¥¼ í†µí•œ LLM í˜¸ì¶œ (í•˜ìœ„ í˜¸í™˜ì„±ì„ ìœ„í•´ ìœ ì§€, ë‚´ë¶€ì ìœ¼ë¡œ call_llm ì‚¬ìš©)"""
    return call_llm(prompt, enable_sequential_thinking=enable_sequential_thinking, use_context7=use_context7)
